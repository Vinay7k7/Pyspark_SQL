{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9418453f-5547-4173-a27e-2b0431746970",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![This is the combination of spark and python !](https://miro.medium.com/v2/format:webp/1*nPcdyVwgcuEZiEZiRqApug.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd21c6ab-3e3a-4f5f-9c14-56ff33238c3f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##üöóThe use üöåof `help()` üöëcommend !_____________üöì__________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69af8473-88e6-4e16-ae59-54d65f811d22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method createDataFrame in module pyspark.sql.session:\n\ncreateDataFrame(data: Union[pyspark.rdd.RDD[Any], Iterable[Any], ForwardRef('PandasDataFrameLike'), ForwardRef('ArrayLike')], schema: Union[pyspark.sql.types.AtomicType, pyspark.sql.types.StructType, str, NoneType] = None, samplingRatio: Optional[float] = None, verifySchema: bool = True) -> pyspark.sql.dataframe.DataFrame method of pyspark.sql.session.SparkSession instance\n    Creates a :class:`DataFrame` from an :class:`RDD`, a list, a :class:`pandas.DataFrame`\n    or a :class:`numpy.ndarray`.\n    \n    .. versionadded:: 2.0.0\n    \n    .. versionchanged:: 3.4.0\n        Support Spark Connect.\n    \n    Parameters\n    ----------\n    data : :class:`RDD` or iterable\n        an RDD of any kind of SQL data representation (:class:`Row`,\n        :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`,\n        :class:`pandas.DataFrame` or :class:`numpy.ndarray`.\n    schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n        a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n        column names, default is None.  The data type string format equals to\n        :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n        omit the ``struct<>``.\n    \n        When ``schema`` is a list of column names, the type of each column\n        will be inferred from ``data``.\n    \n        When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n        from ``data``, which should be an RDD of either :class:`Row`,\n        :class:`namedtuple`, or :class:`dict`.\n    \n        When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must\n        match the real data, or an exception will be thrown at runtime. If the given schema is\n        not :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n        :class:`pyspark.sql.types.StructType` as its only field, and the field name will be\n        \"value\". Each record will also be wrapped into a tuple, which can be converted to row\n        later.\n    samplingRatio : float, optional\n        the sample ratio of rows used for inferring. The first few rows will be used\n        if ``samplingRatio`` is ``None``.\n    verifySchema : bool, optional\n        verify data types of every row against schema. Enabled by default.\n    \n        .. versionadded:: 2.1.0\n    \n    Returns\n    -------\n    :class:`DataFrame`\n    \n    Notes\n    -----\n    Usage with `spark.sql.execution.arrow.pyspark.enabled=True` is experimental.\n    \n    Examples\n    --------\n    Create a DataFrame from a list of tuples.\n    \n    >>> spark.createDataFrame([('Alice', 1)]).collect()\n    [Row(_1='Alice', _2=1)]\n    >>> spark.createDataFrame([('Alice', 1)], ['name', 'age']).collect()\n    [Row(name='Alice', age=1)]\n    \n    Create a DataFrame from a list of dictionaries\n    \n    >>> d = [{'name': 'Alice', 'age': 1}]\n    >>> spark.createDataFrame(d).collect()\n    [Row(age=1, name='Alice')]\n    \n    Create a DataFrame from an RDD.\n    \n    >>> rdd = spark.sparkContext.parallelize([('Alice', 1)])\n    >>> spark.createDataFrame(rdd).collect()\n    [Row(_1='Alice', _2=1)]\n    >>> df = spark.createDataFrame(rdd, ['name', 'age'])\n    >>> df.collect()\n    [Row(name='Alice', age=1)]\n    \n    Create a DataFrame from Row instances.\n    \n    >>> from pyspark.sql import Row\n    >>> Person = Row('name', 'age')\n    >>> person = rdd.map(lambda r: Person(*r))\n    >>> df2 = spark.createDataFrame(person)\n    >>> df2.collect()\n    [Row(name='Alice', age=1)]\n    \n    Create a DataFrame with the explicit schema specified.\n    \n    >>> from pyspark.sql.types import *\n    >>> schema = StructType([\n    ...    StructField(\"name\", StringType(), True),\n    ...    StructField(\"age\", IntegerType(), True)])\n    >>> df3 = spark.createDataFrame(rdd, schema)\n    >>> df3.collect()\n    [Row(name='Alice', age=1)]\n    \n    Create a DataFrame from a pandas DataFrame.\n    \n    >>> spark.createDataFrame(df.toPandas()).collect()  # doctest: +SKIP\n    [Row(name='Alice', age=1)]\n    >>> spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n    [Row(0=1, 1=2)]\n    \n    Create  a DataFrame from an RDD with the schema in DDL formatted string.\n    \n    >>> spark.createDataFrame(rdd, \"a: string, b: int\").collect()\n    [Row(a='Alice', b=1)]\n    >>> rdd = rdd.map(lambda row: row[1])\n    >>> spark.createDataFrame(rdd, \"int\").collect()\n    [Row(value=1)]\n    \n    When the type is unmatched, it throws an exception.\n    \n    >>> spark.createDataFrame(rdd, \"boolean\").collect() # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    Py4JJavaError: ...\n\n"
     ]
    }
   ],
   "source": [
    "#This is a help() commend that halp you with the document of any function ! in pyspark !\n",
    "help(spark.createDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47a465c8-c84c-4126-982c-fb378c07f64b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[2]: pyspark.sql.session.SparkSession"
     ]
    }
   ],
   "source": [
    "type(spark)# this is a object of spark  section !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d363547c-efd9-401f-a185-48c1a9bf45ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n| ID| NAME|\n+---+-----+\n|  1|vinay|\n|  2|kumar|\n+---+-----+\n\nroot\n |-- ID: integer (nullable = true)\n |-- NAME: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "#We can create a dataframe in spark using tuples inside the list !\n",
    "data = [(1,'vinay'),(2,'kumar')]\n",
    "sch = StructType([StructField(name = 'ID',dataType=IntegerType()),#This is a one way of creating the schema for the dataframe !\n",
    "            StructField(name = 'NAME',dataType=StringType())])\n",
    "df = spark.createDataFrame(data=data,schema = sch)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0004f566-ae79-4639-9fb5-635814df15fc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##üöÅIn real life where we need to use the `null values` in the column in the dataframe in that case !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d73c37e-ecb5-488a-8878-9f85b77ed079",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+\n| ID| NAME|     PH_NO|\n+---+-----+----------+\n|  1|Vinay|7013731005|\n|  2|Kumar|9110788277|\n|  3|  sai|1234567897|\n|  4| bala|      null|\n+---+-----+----------+\n\nroot\n |-- ID: integer (nullable = false)\n |-- NAME: string (nullable = false)\n |-- PH_NO: long (nullable = true)\n\n+---+-----+----------+\n| ID| NAME|     PH_NO|\n+---+-----+----------+\n|  1|Vinay|7013731005|\n|  2|Kumar|9110788277|\n|  3|  sai|1234567897|\n|  4| bala|9999999999|\n+---+-----+----------+\n\nroot\n |-- ID: integer (nullable = false)\n |-- NAME: string (nullable = false)\n |-- PH_NO: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#What if there is a need of null values in a column !\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import lit,when\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"example\") \\\n",
    "    .getOrCreate()\n",
    "sch = StructType([StructField(name = 'ID',dataType = IntegerType(),nullable=False),\n",
    "                    StructField(name = 'NAME',dataType = StringType(),nullable=False),\n",
    "                    StructField(name = 'PH_NO',dataType = LongType(),nullable=True)])\n",
    "\n",
    "data = [(1,'Vinay',7013731005),(2,'Kumar',9110788277),(3,'sai',1234567897),(4,'bala',None)]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema = sch)\n",
    "df.show()\n",
    "df.printSchema()\n",
    "\n",
    "\n",
    "#Now if i want to give a default value to the variable then i would use the lit()-> function from \n",
    "#--> \"from pyspark.sql.functions import lit,when\"\n",
    "\n",
    "#Now we can change the null values with the default value !\n",
    "default_value = 9999999999\n",
    "df = df.withColumn('PH_NO',when(df['PH_NO'].isNull(),lit(default_value)).otherwise(df[\"PH_NO\"]))\n",
    "                   \n",
    "df.show()\n",
    "df.printSchema()\n",
    "\n",
    "# This is how we can chage the null values to the default values !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9db6a283-5a69-40b7-bae1-bc1f3e832e4d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##üé∞In this cell we are trying to create a custom data frame with different styles of creatin a schema !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6333631-a049-47cd-889c-030f632cfc79",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n| ID| Name|\n+---+-----+\n|  1|vinay|\n|  2|kumar|\n+---+-----+\n\nroot\n |-- ID: integer (nullable = true)\n |-- Name: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Initialize SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "data = [(1.8978, 'vinay'), (2.9, 'kumar')]\n",
    "\n",
    "\n",
    "\n",
    "# sch = StructType([\n",
    "#     StructField(name='ID', dataType=FloatType()),\n",
    "#     StructField(name='NAME', dataType=StringType())\n",
    "# ])\n",
    "\n",
    "#                       (OR)                --- we can use other way also !\n",
    "\n",
    "sch = StructType().add(field='ID',data_type=FloatType(),nullable=False)\\\n",
    "                .add(field='Name',data_type=StringType(),nullable=True)\n",
    "#This above way much more readable!\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=sch)\n",
    "\n",
    "\n",
    "\n",
    "# Cast the 'ID' column to IntegerType\n",
    "df = df.withColumn(\"ID\", df[\"ID\"].cast(IntegerType()))\n",
    "# This is how we can change the data type of a column after creating the dataframe in pyspark !\n",
    "\n",
    "df.show()# This will show the data_frame !Just like 'print()' in python to shoe data_frames !\n",
    "df.printSchema()# This will print the schema ! of the data_frame !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cab11403-13bf-4d38-b23c-3b29d2e44b63",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##üçîWe can create a data not just using tuples inside a list but we can use the dict inside the list also and for this we don't have to mention \n",
    "\n",
    "##üçüschema because the keys will place as a columns names !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f860516-58bf-47f8-a0a7-9c61ba3a8c0f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n| id| name|\n+---+-----+\n|  1|vinay|\n|  2|kumar|\n|  3| null|\n+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#Now we will create a data_frame data not just from list of tuples !\n",
    "#But using the list of dict!\n",
    "#In this the keys will become the columns names !\n",
    "data = [{'id':1,'name':'vinay'},\n",
    "        {'id':2,'name':'kumar'},\n",
    "        {'id':3,'name':None}]\n",
    "\n",
    "df = spark.createDataFrame(data= data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "daa0ad06-a8a6-4912-9bd7-9f1b3bf7094c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##üé≠This is a `real life situation` !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2d0353a-2585-457f-b9d2-03f75ae4e40d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#NOTE :---\n",
    "#                   ID,Name\n",
    "#                   1,John\n",
    "#                   ,Smith\n",
    "#                   3,Alice\n",
    "#The above thing is the data of a .csv file in that we can see the second row\n",
    "#have a dat abscens in pyspark we need to see the null in data frame if the file data is mentened as None !\n",
    "#But in this the data is just ''-blank !\n",
    "#Then we will do !\n",
    "\n",
    "\n",
    "                                    # from pyspark.sql import SparkSession\n",
    "\n",
    "                                    # # Initialize SparkSession\n",
    "                                    # spark = SparkSession.builder \\\n",
    "                                    #     .appName(\"CSV Example\") \\\n",
    "                                    #     .getOrCreate()\n",
    "\n",
    "                                    # # Read the CSV file and specify the nullValue parameter\n",
    "                                    # df = spark.read.csv(\"sample.csv\", header=True, nullValue=\"\")\n",
    "\n",
    "                                    # df.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a583353f-9db3-411d-9b3a-fb477672525e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##üéüÔ∏èReading the `.csv` file from the pyspark !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "233503f7-1e5a-43e4-80e2-8b17b4537215",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "        #Now we will see how to read the .csv files into pyspark !\n",
    "# file_location = \"/FileStore/tables/prac_dataset_-1.csv\"\n",
    "df = spark.read.csv(path = \"/FileStore/tables/prac_dataset_-1.csv\",header=True,inferSchema=True)\n",
    "# display(df) #This will display in the tabler form !\n",
    " \n",
    "# df.printSchema()\n",
    "#df.show() #This will show the data in the raw form !\n",
    "df.head(3)#This will provide the list of values !\n",
    "k1 = df.columns\n",
    "# print(k1[4:7])\n",
    "dff = df.select(k1[3:8]).limit(3)#this is how we can specifi the no_of rows \n",
    "#But it is just from the start !\n",
    "dff1 = df.select(k1[3:8]).collect()[3:8] #This is used to retrive the bunch of rows in the any place from the data_frame !\n",
    "# dff.show()\n",
    "display(dff)\n",
    "display(dff1)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32f12889-04f7-4c61-8de0-3e921fcc7469",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##üóÉÔ∏èThis is the other way to acessing the file !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06d20f09-8aa6-459d-8931-d2741bbe8c6a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#This is the other way to read the data too\n",
    "df =spark.read.format('csv')\\\n",
    "                            .option('header',True)\\\n",
    "                            .load('/FileStore/tables/prac_dataset_-1.csv')\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fa81cbb-1579-4640-ac03-42125504d67b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##‚òÑÔ∏èHow to read/write dataframes into the CSV file using the PySpark !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b545761b-2f37-42ec-a276-7f565fa897be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "\n",
    "data = [(1,'vinay'),(2,'kumar')]\n",
    "sch = ['id','name']\n",
    "df_1 =spark.createDataFrame(data = data,schema = sch)\n",
    "df_1.show()\n",
    "\n",
    "\n",
    "df.write.csv(path='dbfs:/FileStore/samp/r1',header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03e0511f-d82d-4326-9c8b-187a330bc6d7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##üò∂‚Äçüå´Ô∏èThis is a very Imp concept called `Mode` while writing the data to the file path !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e06bc98c-126e-4231-9386-e4d4108602b6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "ü™ÇWe will get the error because on the concept called 'mode'\n",
    "The mode is nothing but the attribute in the df.write.csv() there are \n",
    "four types of modes !\n",
    "1) --> append this will appends the newley given data to file path !\n",
    "2) --> ignore this will not rise any error if the file is already in that given file path !\n",
    "3) --> overwrite this will overwrite the csv file in that given file path !\n",
    "4) --> error this is a {default} if we havent specified any mode then pyspark will take this this will rise the error if the file is exist in the given file path !\n",
    "#####üß®And also there is also an IMP concept to know when ever we write the data or a data_frame in code and tries to store in the file system by using the above codeing part\n",
    "#####üß®But when we go and see the actual structer of the file that have been stored then we can see lot of part's of the file that u have uploded int that file path\n",
    "#####üß®Because of the concept of {The master and -the worker nodes} along with that files (parted) you will find the log files and some other files to those are created by spark !\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4412ea70-ea16-4cae-9851-4c0be3ca17c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.write.csv(path='dbfs:/FileStore/samp/r1',header = True)#,mode='overwrite'\n",
    "#IF we run the above code then it will throw the error \"AnalysisException: Path dbfs:/FileStore/samp already exists.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dba61423-d97a-4fc0-aca9-e57cd2575352",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Let extract the data from the created .csv file !\n",
    "display(spark.read.csv(path=\"dbfs:/FileStore/samp/r1\"),header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79b3bfdf-f138-42ac-b1ce-58dd8f5349f1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##üîûHow to read the json files in to the pyspark and aslo we will know how to write into the json file !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67a5a499-e418-4fa4-9011-c1d7130e9fe9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##NOTE:-\n",
    "          In this we can use the parameter { multiLine = } what it is used for means that \n",
    "          when ever we are having the json data in this formate !\n",
    "          {'id':1,'name':'vinay','age':23}\n",
    "          {'id':2,'name':'kumar','age':32}\n",
    "\n",
    "\n",
    "          just the the above format if the json data is then the multiLine = False !\n",
    "\n",
    "          If the data format is like this \n",
    "          {\n",
    "          'id':1,\n",
    "          'name':'vinay',\n",
    "          'age':23\n",
    "          }\n",
    "          {\n",
    "          'id':2,\n",
    "          'name':'kumar',\n",
    "          'age':32\n",
    "          }\n",
    "\n",
    "\n",
    "          If the above format is like this then the multiLine = True !\n",
    "\n",
    "\n",
    "          It helps in reading the format in the json file !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34045ccc-5be1-4ead-bbde-24cc652d06d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Now we will read the data outof the given file path !\n",
    "from pyspark.sql.types import *\n",
    "sch = StructType().add(field = 'ID',data_type = IntegerType(),nullable = True)\\\n",
    "                    .add(field = 'Name',data_type = IntegerType(),nullable = True)\\\n",
    "                    .add(field='AGE',data_type=StringType(),nullable=True)\n",
    "df_json = spark.read.json(path = 'dbfs:/FileStore/New_Text_Document.json',schema= sch,multiLine = False)\n",
    "df_json.printSchema()\n",
    "df_json.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afe7a383-9894-464f-b2d8-f6e503c8e9be",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##üå≠There is an other way of reading the json file !\n",
    "####üçûThat is using the .format() --> function !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37c9356c-0195-4278-92e7-f89f5c3e908b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "df = spark.read.format('json')\\\n",
    "                    .option('schema',sch)\\\n",
    "                    .load('dbfs:/FileStore/New_Text_Document.json')\n",
    "df.show()#This .show() will show the first 20 characters from the data if your data is much longer than that then it will show first '20 char...'\n",
    "#In this .show() there is attributes like 'n = '20'' --> this will tell how many rows to print!\n",
    "#'truncate = 'True' or 'int' --> this will take an int or bool as a parameter it will turncata a data in the given length if not it will truncate the length of the data in 20 if falst it will show all the data with out truncating the data ! --> by defalut it will be 20 !\n",
    "#'vertical = 'False'' then it will show the schema in the normaly way but if u give the parameter as the 'True' --> by default it will be \"False\"\n",
    "df.printSchema()\n",
    "display(df)#This will the display the data in nice tabler format !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "303e9524-3b7d-451b-bb7f-117ff41ad5ba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##ü•©How to create a json data and puting the data in the file !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "327a025f-0398-42e9-b052-4e06e100c17d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data =[(1,'vinay'),(2,'kumar')]\n",
    "sch = ['id','name']\n",
    "df_1 = spark.createDataFrame(data = data,schema = sch)\n",
    "df_1.write.json('dbfs:/FileStore/json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b8ef734-99ba-46d7-9184-5f4317a8b7ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#In this cell i am gonna adda new data to the same location ! with mode = append !\n",
    "data_1 = [(3,'sai'),(4,'raj')]\n",
    "sch_1 = ['id','name']\n",
    "df_2 = spark.createDataFrame(data = data_1,schema=sch_1)\n",
    "df_2.write.json(\"dbfs:/FileStore/json\",mode = 'append')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2030a857-7bd0-4c87-a244-87ac4ad7903b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Reading the Data from the json ! where we have stored !\n",
    "gg = spark.read.format('json')\\\n",
    "                .option('Header',True)\\\n",
    "                .load('dbfs:/FileStore/json')\n",
    "display(gg)                \n",
    "\n",
    "#After append new data we can see the new data has been added !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f71375dc-c259-4405-b1f6-a7df8fc8efaa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##üç∞How to read the parquet file and store the data into the parquet file !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33fd54fb-3374-4e6d-b057-d32a548136bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.read.parquet('dbfs:/FileStore/json/mt_cars.parquet',header=True)\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d402d90-2fcb-413b-8735-fced8877f2d7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##üçñHow to add the data into the parquet file !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "340077f1-fe7c-419d-a742-29f1aeadff58",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [(1,'vinay'),(2,'kumar')]\n",
    "sch = ['id','name']\n",
    "df = spark.createDataFrame(data = data,schema=sch)\n",
    "display(df)\n",
    "df.write.parquet('dbfs:/FileStore/parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03bfdf5b-5d78-48cf-91c1-ae1a052f0df3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#now we will add the new data to the parquet file path ! with mode = append !\n",
    "data_1  = [(3,'sai'),(4,'raj')]\n",
    "sch = ['id','name']\n",
    "df_1 = spark.createDataFrame(data = data_1,schema = sch_1)\n",
    "df_1.write.parquet('dbfs:/FileStore/parquet',mode = 'append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38fe969f-ccf1-4175-9618-956d30fba662",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#After adding the new data i would like to see the data !\n",
    "display(spark.read.parquet('dbfs:/FileStore/parquet'))\n",
    "#There u can see the newly added data !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7ae18d8-edc2-440a-b2e9-2d6b0700f531",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#This below peace of code is used to remove any file path that is given !\n",
    "\n",
    "from pyspark.dbutils import DBUtils\n",
    "\n",
    "dbutils = DBUtils(spark)\n",
    "fp = 'dbfs:/FileStore/json/mt_cars.parquet'\n",
    "dbutils.fs.rm(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4935c613-aedd-4475-b8df-20e544366909",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##üê∏Now we will know the use case of the .show() function !!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d488a5b4-9ec0-4141-89ee-e7e58acca5e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#.show() ---> function\n",
    "\n",
    "df.show(n=1,vertical =True,truncate = 2)\n",
    "\n",
    "#This .show() will show the first 20 characters from the data if your data is much longer than that then it will show first '20 char...'\n",
    "#In this .show() there is attributes like 'n = '20'' --> this will tell how many rows to print!\n",
    "#'truncate = 'True' or 'int' --> this will take an int or bool as a parameter it will turncata a data in the given length if not it will truncate the length of the data in 20 if falst it will show all the data with out truncating the data ! --> by defalut it will be 20 !\n",
    "#'vertical = 'False'' then it will show the schema in the normaly way but if u give the parameter as the 'True' --> by default it will be \"False\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbf52c95-2ae7-4d41-986e-ca8a492f57ba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##üß†Now we will see how to add a new column or change values in a column or chage the column datatype in dataframe !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70867cf8-97d7-45e6-b2a2-c621b536c0db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,lit\n",
    "data  = [(1,'vinay',25000),(2,'kumar',34000),(3,'sai',43000)]\n",
    "sch  =['ID','Name','Salary']\n",
    "df = spark.createDataFrame(data = data,schema = sch)\n",
    "df.show()\n",
    "df.printSchema()#In this result the column 'salary datatype is showing as 'long !'' but we need to change the data type in that column \n",
    "#yes we can do that using defining the required data schema but apart from that we can also do this !\n",
    "df = df.withColumn(colName ='salary',col = col('salary').cast('Integer'))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d1f2e56-5f3b-48b3-9755-62e6637f7d16",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#In the above cell we have used the method col() to read a column from the data frame but we can use <data_frame['<column_name>']> this will also work !\n",
    "\n",
    "#In the above example i have changed the data type of a specific column in the dataframe !\n",
    "#Now i need to change the values in the column in the dataFrame !\n",
    "df_2 = df.withColumn('salary',col('salary')+4.87)\n",
    "df_2.show()\n",
    "df_2 = df_2.withColumn('salary',col('salary').cast('Float'))\n",
    "df_2.printSchema()\n",
    "\n",
    "#From this i have learned that we cannot do the casting and the data upgrade at the same time !\n",
    "\n",
    "#now what if i need to add a new colum to the dataframe !! then \n",
    "df_3 = df_2.withColumn('country',lit('India'))#at first it will check for the given column in the dataframe if not found it will create the column \n",
    "#with that name and to pass a data in to that column we will use the function 'lit()' in this we can give the data !\n",
    "df_3.show()\n",
    "\n",
    "#Ok now i woul like to change name of the column in the dataframe !! then\n",
    "df_4 = df_3.withColumnRenamed('salary','payment_for_their_work')\n",
    "df_4.show()\n",
    "#Using the .withColumnRenamed() function we can actully change the column name that is there in the dataframe !\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e58c6e2-3563-4f7a-8ce0-955f2a1e0097",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##ü•∏In this we will learn a little more about creating schema !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea75597a-8033-4d00-95e3-c3b2a534e90b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+------+\n| Id|          Name|Salary|\n+---+--------------+------+\n|  1|{vinay, kumar}| 23000|\n|  2|   {sai, ravi}| 67000|\n+---+--------------+------+\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Id</th><th>Name</th><th>Salary</th></tr></thead><tbody><tr><td>1</td><td>List(vinay, kumar)</td><td>23000</td></tr><tr><td>2</td><td>List(sai, ravi)</td><td>67000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         [
          "vinay",
          "kumar"
         ],
         23000
        ],
        [
         2,
         [
          "sai",
          "ravi"
         ],
         67000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Name",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"first_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"last_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "Salary",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Id: integer (nullable = true)\n |-- Name: struct (nullable = true)\n |    |-- first_name: string (nullable = true)\n |    |-- last_name: string (nullable = true)\n |-- Salary: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "#lets know a little more about the creating the schema for the data !\n",
    "data = [(1,('vinay','kumar'),23000),(2,('sai','ravi'),67000)]\n",
    "#if we cleary observe the data structer then you might find a tuple inside a tuple in this situation how can we create a schema for such data !\n",
    "inner_sch = StructType().add(field = 'first_name',data_type = StringType())\\\n",
    "                        .add(field = 'last_name',data_type = StringType())\n",
    "outer_sch = StructType().add(field = 'Id',data_type=IntegerType())\\\n",
    "                        .add(field = 'Name',data_type = inner_sch)\\\n",
    "                        .add(field = 'Salary',data_type = IntegerType())\n",
    "\n",
    "df = spark.createDataFrame(data = data,schema = outer_sch)\n",
    "df.show()\n",
    "display(df)\n",
    "df.printSchema()\n",
    "\n",
    "#This is how you have to deal with the nested part of the data !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a325632-8678-4ec6-92c6-ebb5a3fa8cc0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##‚ôªÔ∏èNow we will learn the Array type in the pyspark !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccd1a422-7c14-4b85-8f8a-9e0895a81f07",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "data = [('abc', [1,2]),('zyx',[3,4]),('jlk',[6,7])]\n",
    "sch = ['id','numbers']\n",
    "df = spark.createDataFrame(data = data , schema = sch )\n",
    "df.show()\n",
    "df.printSchema()\n",
    "\n",
    "\n",
    "#In the schema it is showing the arrayType() but in side that by default it is showing the data type a long we can change that using !\n",
    "sch = StructType().add(field = 'Id',data_type = StringType())\\\n",
    "                    .add(field = 'Number',data_type = ArrayType(IntegerType()))\n",
    "df_1 = spark.createDataFrame(data = data,schema = sch)\n",
    "df_1.show()\n",
    "df_1.printSchema()\n",
    "#This is how we can make use of the array datatype !\n",
    "#The difference between inner tuple and the inner array is that for tuple'()' and this is for '[]' array and also for inner tuple the dataType is Structer\n",
    "#But when it comes to the list inside the tuple then the type would be array not structer !\n",
    "\n",
    "#And also we can acess the elementd in side the array elements from the indexing !\n",
    "\n",
    "df_1  = df_1.withColumn('firstNumber',df_1.Number[0])#This line will creata a column that will take the first element of the Array as a data in it !\n",
    "\n",
    "display(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83f5c0a4-dee3-4465-9f0a-38e4edebd9a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+--------+\n|num_1|num_2| numbers|\n+-----+-----+--------+\n|    1|    3|  [1, 3]|\n|   53|   73|[53, 73]|\n|   32|   74|[32, 74]|\n|    3|   35| [3, 35]|\n+-----+-----+--------+\n\n"
     ]
    }
   ],
   "source": [
    "#Not only we can extract the data from the array but we can create the array and add that data to the new column !\n",
    "from pyspark.sql.functions import *\n",
    "data = [(1,3),(53,73),(32,74),(3,35)]\n",
    "sch  = ['num_1','num_2']\n",
    "df = spark.createDataFrame(data = data,schema = sch)\n",
    "#now i need to add a column that stires the array of the values of num_1 and num_2 in that array !\n",
    "df = df.withColumn('numbers',array(df.num_1,df.num_2)).show() #in order to work with array() function we need to import that !\n",
    "#from the above column wee can see that we can use col(<column_name>) or df['<column_name>'] or <data_frame>.<column_name>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69eb3cda-60ca-494b-98d9-3afa0bd04fbe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Now we use the functions like explode(),split(),array() and array_contains() functions in pyspark !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b13e41bf-e607-46d5-898c-6050c69847e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "data  = [(1,'vinay',['java','python']),(2,'kumar',['c','c++']),(3,'sai',['javascript','python','c'])]\n",
    "sch = ['Id',\"Name\",'Skills']\n",
    "df = spark.createDataFrame(data = data ,schema = sch)\n",
    "df = df.withColumn('Id',df.Id.cast(IntegerType()))#This is used to change the dataType()\n",
    "df.show()\n",
    "df.printSchema()\n",
    "\n",
    "#Now we will see how the explode() function is used !\n",
    "#In the Array there are elements in the skills column in the data_frame but if i need the column to show the every data in the array indualy in the column then we will \n",
    "#use the explode(<column_name>)\n",
    "df_1 = df.withColumn('skills',explode(df.Skills))\n",
    "df_1.show()\n",
    "#In the output you can see a lot more lines the dataframe this is because of the explode and it will print the every element in the array into the new row \n",
    "#Just like this we can also create a new column and do this in that column !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85a50c26-bd94-48ea-a15b-ad72ca1ce167",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Now we will see how the split() function is used !\n",
    "#This split() is used when the given data is a single string with out using a array \n",
    "#like '.net,python,java' rather then using a array like this ['.net','python','java']\n",
    "\n",
    "#In this kind of situatins we can use this split() function !\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "data = [(1,'vinay','.net,python,java'),(2,'kumar','c,java,javascript'),(3,'sai','HTML,css,javascript')]\n",
    "sch = ['Id','Name','Skills']\n",
    "df = spark.createDataFrame(data = data,schema = sch)\n",
    "df = df.withColumn('Skills',split(df.Skills,','))#In this place we are spliting the data in the Skills !\n",
    "\n",
    "df.show(truncate=50)\n",
    "df.printSchema()\n",
    "df = df.withColumn('Id',df.Id.cast(IntegerType()))\n",
    "df.show(truncate=50)\n",
    "df.printSchema()\n",
    "#Now we will se how to use the array_contains() !\n",
    "data  = [(1,'vinay',['java','python']),(2,'kumar',['c','c++']),(3,'sai',['javascript','python','c'])]\n",
    "sch = ['Id',\"Name\",'Skills']\n",
    "df = spark.createDataFrame(data = data,schema = sch)\n",
    "df.show()\n",
    "df_1 = df.withColumn('Skill',array_contains(df.Skills,'python'))#In this the array_contains() we will take a column and a value \n",
    "#Then it will check if the given value is in the provided column and if it is there the 'True' if not 'False' even the data is 'None' it is 'False'\n",
    "#it will even create a new column if u want to\n",
    "df_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd6d9b96-51c9-4b5c-b21e-1007c3cfb3af",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Now we will use the `MapType` columns in pyspark !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30b438b4-a9cd-4954-b4de-cace96656919",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#The mapType is used to give a dataType to the dictionary type data in the dataFrame !\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "data = [(1,'vinay',{'city':'kurnool','town':'Nandikotkur'}),(2,'kumar',{'city':'Nandyal','town':'Nandyal'}),(3,'sai',{'city':'anathapur','town':'koyalakuntla'})]\n",
    "sch = StructType().add(field = 'Id',data_type=IntegerType())\\\n",
    "                    .add(field = 'Name',data_type=StringType())\\\n",
    "                    .add(field='Address',data_type=MapType(StringType(),StringType()))\n",
    "df = spark.createDataFrame(data = data,schema = sch)\n",
    "df.show()\n",
    "df.printSchema()\n",
    "#The data in the dataframe is using the dictionary then for the creation of the schema for that dataframe we can use MapType() to represent the dictionary dataformat\n",
    "#But what if the dict data is a mixed dict then like --> \"data = [(1,'vinay',{'city':'kurnool','age':23}),(2,'kumar',{'city':'Nandyal','age':65}),(3,'sai',{'city':'anathapur','age':44})]\"\n",
    "#Then the StringType() can store both the string and as well as the integer and float type as well !\n",
    "\n",
    "\n",
    "#NOTE:-\n",
    "#If u use a tupe() in side the dataframe as a nested ! then the dataType that we use for it is StructType()\n",
    "#If u use a list[] in side the dataframe as a nested ! then the dataType that we use for it is ArrayType()\n",
    "#If u use a dict{} in side the dataframe as a nested ! then the dataType that we use for it is MapType()\n",
    "#just like the array we will access the data inside the dict !\n",
    "\n",
    "df = df.withColumn('City_',df.Address['city'])#This is the one way of acessing the values in the dict from the dataFrame !\n",
    "df.show(truncate = 50)\n",
    "df = df.withColumn('Town_',df.Address.getItem('town'))#This is the other way of getting the values from the dataFrame !\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51d43a48-f616-475b-b081-9d3120101b27",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Now we use `map_keys()`,`map_values()`,`explode()` in the pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "352c982d-63f3-4511-9e13-78c56d9f6134",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------------+\n| Id| Name|             Address|\n+---+-----+--------------------+\n|  1|vinay|{town -> Nandikot...|\n|  2|kumar|{town -> Nandyal,...|\n|  3|  sai|{town -> koyalaku...|\n+---+-----+--------------------+\n\nroot\n |-- Id: integer (nullable = true)\n |-- Name: string (nullable = true)\n |-- Address: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n+---+-----+-----------------------------------------+----+------------+\n|Id |Name |Address                                  |key |value       |\n+---+-----+-----------------------------------------+----+------------+\n|1  |vinay|{town -> Nandikotkur, city -> kurnool}   |town|Nandikotkur |\n|1  |vinay|{town -> Nandikotkur, city -> kurnool}   |city|kurnool     |\n|2  |kumar|{town -> Nandyal, city -> Nandyal}       |town|Nandyal     |\n|2  |kumar|{town -> Nandyal, city -> Nandyal}       |city|Nandyal     |\n|3  |sai  |{town -> koyalakuntla, city -> anathapur}|town|koyalakuntla|\n|3  |sai  |{town -> koyalakuntla, city -> anathapur}|city|anathapur   |\n+---+-----+-----------------------------------------+----+------------+\n\n+---+-----+--------------------+------------+\n| Id| Name|             Address|        keys|\n+---+-----+--------------------+------------+\n|  1|vinay|{town -> Nandikot...|[town, city]|\n|  2|kumar|{town -> Nandyal,...|[town, city]|\n|  3|  sai|{town -> koyalaku...|[town, city]|\n+---+-----+--------------------+------------+\n\n+---+-----+-----------------------------------------+------------+-------------------------+\n|Id |Name |Address                                  |keys        |values                   |\n+---+-----+-----------------------------------------+------------+-------------------------+\n|1  |vinay|{town -> Nandikotkur, city -> kurnool}   |[town, city]|[Nandikotkur, kurnool]   |\n|2  |kumar|{town -> Nandyal, city -> Nandyal}       |[town, city]|[Nandyal, Nandyal]       |\n|3  |sai  |{town -> koyalakuntla, city -> anathapur}|[town, city]|[koyalakuntla, anathapur]|\n+---+-----+-----------------------------------------+------------+-------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "data = [(1,'vinay',{'city':'kurnool','town':'Nandikotkur'}),(2,'kumar',{'city':'Nandyal','town':'Nandyal'}),(3,'sai',{'city':'anathapur','town':'koyalakuntla'})]\n",
    "sch = StructType().add(field = 'Id',data_type=IntegerType())\\\n",
    "                    .add(field = 'Name',data_type=StringType())\\\n",
    "                    .add(field='Address',data_type=MapType(StringType(),StringType()))\n",
    "df = spark.createDataFrame(data = data,schema = sch)\n",
    "df.show()\n",
    "df.printSchema()                    \n",
    "df_1 = df.select('Id','Name','Address',explode(df['Address']))#when exploding thw dict it will give two values 'kays' and 'values' so need to create two columns !\n",
    "df_1.show(truncate = False)\n",
    "#this is how we will use the explode() function on the mapType() using the select()\n",
    "#And also the select is a function that selects the columns that has been given as a parameters and will shows those colmns and we can also add the condition if u want to !\n",
    "\n",
    "#If we closley observe the dataframe you can see the newly added colunmns are storing the keys and values in each supperate row !\n",
    "#what if we need to store the keys of the MapType() in a list in that row !\n",
    "#For that !\n",
    "df_2 = df.withColumn('keys',map_keys(df.Address))\n",
    "df_2.show() #In this we ccan see a newly added columns holds a array of keys in that MapType()\n",
    "\n",
    "\n",
    "#What if i need to store the values of the MapType() in a list in a new column in the dataFrame !\n",
    "df_3 = df_2.withColumn('values',map_values(df.Address))\n",
    "df_3.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9924a84-c2bd-460b-926a-07dd16c6d01a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Let use use the `Row() class` in Pyspark !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94b21bf3-705f-4230-b3b9-f6ecce104bdb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\nVinay\nVinay 34\nKumar\n+-----+---+------+\n| Name|Age| skill|\n+-----+---+------+\n|vinay| 23|Python|\n|Kumar| 33|  java|\n+-----+---+------+\n\nroot\n |-- Name: string (nullable = true)\n |-- Age: long (nullable = true)\n |-- skill: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "#The Row() is nothing a function that creates the row in a dataFrame just like a list!\n",
    "k1 = Row('vinay',23)\n",
    "print(k1[1])#This is how you can access the values in the Row() function just like List[]\n",
    "\n",
    "#But what makes it different from List is that you can provide variables or keys to those values inside the Row() function !\n",
    "k2 = Row(name ='Vinay',age = 34)\n",
    "print(k2.name)#By just calling the name of the key we can access the value of that key !\n",
    "print(k2.name +' '+ str(k2.age) )\n",
    "#If u see colsley you can spot a difference ! that is it act's like list and also like dict but in this we will create a variales but not keys and we also use '=' not ':' like in dict's\n",
    "\n",
    "#The good thing about this Row() function is that we can use it to create the class in this !\n",
    "\n",
    "k = Row('Name','Age','skill')#In this k is like a class and the name ,age and skills are the attributes in that class \n",
    "k1 = k('vinay',23,'Python')#The k1 is a object to this class and in this we are sending the data to that variables in that class\n",
    "k2 = k('Kumar',33,'java')\n",
    "\n",
    "print(k2.Name)#This is how we can access the values based on the object that we have created to that class !\n",
    "\n",
    "#And we can make this oblects into a data frame !\n",
    "\n",
    "df = spark.createDataFrame([k1,k2])#This is how we will use the Row() to create rows in the dataFrame !\n",
    "df.show()\n",
    "df.printSchema()\n",
    "\n",
    "\n",
    "#We can use this Row() function while defining the inner tupe like structType() you can use Row() or just () in to the data !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f7a0f8e-afa7-445a-a455-8d5ae4ad8f68",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Let us use the `Column class` in pyspark !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "daf1ca3e-c24a-4a77-9285-ee13acdcfe77",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "      This is a vary important thing to know that is we cannot add a outer values in to the array or a tuple or a dict in to the column of a \n",
    "      dataframe but we can use the values that are already in the dataframe to add those values in to the array or a tuple or a dict !\n",
    "\n",
    "      But when it comes to the external data for adding the values in to the column is differentily done like this\n",
    "      if i have a tuple(1,2,3)\n",
    "      and i wanted to creata a new column and i want to put this tuple in to that column and for that the dataType() have to be a StructType()\n",
    "\n",
    "      it can be done by \n",
    "            using struct() and lit() the lit() is used to pust a single value in and the struct() will create a tuple of values !!\n",
    "            k1 =(1,2,3)\n",
    "            struct([lit(i) for i in k1]) #This is how we need to pass a each value to the list ! \n",
    "\n",
    "\n",
    "      just like this we need to work for the array also and for adding the dict to the column we need to use the create_map() function with lit()\n",
    "      for this also we need to pass the list of keys and values like\n",
    "\n",
    "      a dict --> sam = {'height':158,'gender':'M'}\n",
    "      we need to make the dict in to a list ll=['height', 158, 'gender', 'M'] and pass those each value to the method like creat_map([lit(i) for i in ll])\n",
    "      this is how we can be able to put the dict into the column !!!\n",
    "      and i have used the chain function from itertools to split the keys and values from the dict to the values of the list !\n",
    "      kk = list(chain(*sam.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3668085-5bf4-4e59-a9ad-0b2a18a4cef1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n| ID| Name|Salary|\n+---+-----+------+\n|  1|vinay| 25000|\n|  2|kumar| 34000|\n|  3|  sai| 43000|\n+---+-----+------+\n\nroot\n |-- ID: long (nullable = true)\n |-- Name: string (nullable = true)\n |-- Salary: long (nullable = true)\n\n+---+-----+------+------------+\n| ID| Name|Salary|  New_column|\n+---+-----+------+------------+\n|  1|vinay| 25000|{red, black}|\n|  2|kumar| 34000|{red, black}|\n|  3|  sai| 43000|{red, black}|\n+---+-----+------+------------+\n\nroot\n |-- ID: long (nullable = true)\n |-- Name: string (nullable = true)\n |-- Salary: long (nullable = true)\n |-- New_column: struct (nullable = false)\n |    |-- col1: string (nullable = false)\n |    |-- col2: string (nullable = false)\n\n"
     ]
    }
   ],
   "source": [
    "#This is adding the tuple or StructType() data into the column !\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "data  = [(1,'vinay',25000),(2,'kumar',34000),(3,'sai',43000)]\n",
    "sch  =['ID','Name','Salary']\n",
    "df = spark.createDataFrame(data = data,schema = sch)\n",
    "df.show()\n",
    "df.printSchema()\n",
    "#Now we will add a new column for the above dataFrame !\n",
    "sam = ('red','black')\n",
    "df = df.withColumn('New_column',struct([lit(x) for x in sam]))#this is how we can made a structType in side the column !\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecb9605b-cc9b-4725-89c7-8c124a213cdb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n| ID| Name|Salary|\n+---+-----+------+\n|  1|vinay| 25000|\n|  2|kumar| 34000|\n|  3|  sai| 43000|\n+---+-----+------+\n\nroot\n |-- ID: long (nullable = true)\n |-- Name: string (nullable = true)\n |-- Salary: long (nullable = true)\n\n+---+-----+------+------------+\n| ID| Name|Salary|  New_column|\n+---+-----+------+------------+\n|  1|vinay| 25000|[red, black]|\n|  2|kumar| 34000|[red, black]|\n|  3|  sai| 43000|[red, black]|\n+---+-----+------+------------+\n\nroot\n |-- ID: long (nullable = true)\n |-- Name: string (nullable = true)\n |-- Salary: long (nullable = true)\n |-- New_column: array (nullable = false)\n |    |-- element: string (containsNull = false)\n\n"
     ]
    }
   ],
   "source": [
    "#This is adding the array or ArrayType() data into the column !\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "data  = [(1,'vinay',25000),(2,'kumar',34000),(3,'sai',43000)]\n",
    "sch  =['ID','Name','Salary']\n",
    "df = spark.createDataFrame(data = data,schema = sch)\n",
    "df.show()\n",
    "df.printSchema()\n",
    "#Now we will add a new column for the above dataFrame !\n",
    "sam = ['red','black']\n",
    "df = df.withColumn('New_column',array([lit(x) for x in sam]))#this is how we can made a arrayType in side the column !\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3705e51-0b59-47ab-8e0c-bb4d928b55e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n| ID| Name|Salary|\n+---+-----+------+\n|  1|vinay| 25000|\n|  2|kumar| 34000|\n|  3|  sai| 43000|\n+---+-----+------+\n\nroot\n |-- ID: long (nullable = true)\n |-- Name: string (nullable = true)\n |-- Salary: long (nullable = true)\n\n+---+-----+------+--------------------+\n| ID| Name|Salary|          New_column|\n+---+-----+------+--------------------+\n|  1|vinay| 25000|{height -> 158, g...|\n|  2|kumar| 34000|{height -> 158, g...|\n|  3|  sai| 43000|{height -> 158, g...|\n+---+-----+------+--------------------+\n\nroot\n |-- ID: long (nullable = true)\n |-- Name: string (nullable = true)\n |-- Salary: long (nullable = true)\n |-- New_column: map (nullable = false)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = false)\n\n['height', 158, 'gender', 'M']\n"
     ]
    }
   ],
   "source": [
    "#This is adding the dict or MapType() data into the column !\n",
    "\n",
    "rom pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from itertools import chain\n",
    "data  = [(1,'vinay',25000),(2,'kumar',34000),(3,'sai',43000)]\n",
    "sch  =['ID','Name','Salary']\n",
    "df = spark.createDataFrame(data = data,schema = sch)\n",
    "df.show()\n",
    "df.printSchema()\n",
    "#Now we will add a new column for the above dataFrame !\n",
    "sam = {'height':158,'gender':'M'}\n",
    "kk = list(chain(*sam.items()))\n",
    "# samm = str(sam)\n",
    "df = df.withColumn('New_column',create_map([lit(i) for i in kk]))\n",
    "#Thi sis how we are adding the dict values to the column !\n",
    "df.show()\n",
    "df.printSchema()\n",
    "print(kk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4bcfc90e-82e4-41f1-8189-cff5711d40d4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Now lets use `when()` and `otherwise()` function in pyspark !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c52447fb-99c4-4f25-a933-5ddf92e77276",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+------+\n| Id| Name|Gender|Salary|\n+---+-----+------+------+\n|  1|vinay|     M| 50000|\n|  2|kumar|      | 43000|\n+---+-----+------+------+\n\n+---+-----+-------+------+\n| Id| Name| Gender|Salary|\n+---+-----+-------+------+\n|  1|vinay|   Male| 50000|\n|  2|kumar|Unknown| 43000|\n+---+-----+-------+------+\n\nroot\n |-- Id: long (nullable = true)\n |-- Name: string (nullable = true)\n |-- Gender: string (nullable = false)\n |-- Salary: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "#The when() and otherwise() function are used as a cundictional statements just like if ans if_else !\n",
    "data = [(1,'vinay','M',50000),(2,'kumar','',43000)]\n",
    "sch = ['Id','Name','Gender','Salary']\n",
    "df = spark.createDataFrame(data,sch)\n",
    "df.show()\n",
    "df = df.select('Id','Name',when(df.Gender.isin('M','m'),'Male').when(df.Gender == 'F','Femail').otherwise('Unknown').alias('Gender'),'Salary')\n",
    "#Inside the condition if u are comparibg with a single value then 'x == 1' id ok but if u want to check if a value is in a list or not then we can\n",
    "#use this .isin() function ! and .alias() is used to give a name ! !\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a3e28dd-e589-4142-b77b-e264adf12659",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Now we will know the `alias()` and `asc()` and `desc()` and `cast()` and `like()` on a column in the dataframe in pyspark!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24e8a123-0db0-4a5c-bef2-e9766a98298b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n| Id| Name|Salary|\n+---+-----+------+\n|  1|vinay| 50000|\n|  2|kumar| 43000|\n+---+-----+------+\n\nroot\n |-- Id: long (nullable = true)\n |-- Name: string (nullable = true)\n |-- Salary: long (nullable = true)\n\n+---+\n|mad|\n+---+\n|  1|\n|  2|\n+---+\n\n+---+-----+------+\n| Id| Name|Salary|\n+---+-----+------+\n|  2|kumar| 43000|\n|  1|vinay| 50000|\n+---+-----+------+\n\n+---+-----+------+\n| Id| Name|Salary|\n+---+-----+------+\n|  1|vinay| 50000|\n|  2|kumar| 43000|\n+---+-----+------+\n\nroot\n |-- Id: integer (nullable = true)\n |-- Name: string (nullable = true)\n |-- Salary: integer (nullable = true)\n\n+-------------+\n|Name LIKE _u%|\n+-------------+\n|        false|\n|         true|\n+-------------+\n\n+---+-----+------+\n| Id| Name|Salary|\n+---+-----+------+\n|  2|kumar| 43000|\n+---+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [(1,'vinay',50000),(2,'kumar',43000)]\n",
    "sch = ['Id','Name','Salary']\n",
    "df = spark.createDataFrame(data,sch)\n",
    "df.show()\n",
    "df.printSchema()\n",
    "#Use of the .alias()\n",
    "df_1 = df.select(df.Id.alias('mad'))#In order to perform the .alias() function we need to give like this df.Id.alias() not 'Id'\n",
    "df_1.show()\n",
    "#Use of .asc() and .desc()\n",
    "df_2 = df.sort(df.Name.asc())#This will show all the data frame but sort the dataFrame with a column in asc() or desc()\n",
    "df_2.show()\n",
    "\n",
    "#Use of .cast()\n",
    "df_3 = df.select(df.Id.cast('Int'),df.Name,df.Salary.cast(IntegerType())) #In this you can use .cast('Int') or .cast(IntegerType())\n",
    "df_3.show()\n",
    "df_3.printSchema()\n",
    "\n",
    "#Use of .like()\n",
    "df_4 = df.select(df.Name.like('_u%'))#This will only show the boolien data in the table and also you only get that only column !\n",
    "# to get the actiual data fro that like you can use .filter()\n",
    "df_4.show()\n",
    "\n",
    "df_5 = df.filter(df.Name.like('_u%'))#This will give all the dataframe data based on the .like() function !\n",
    "df_5.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02e10d29-e311-4bf6-910c-9014145fa1e4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Now we will use `filter()` and `where()` function in pyspark !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4889bafe-5828-4e28-b3b1-658abd40aa34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+------+\n| Id| Name|Gender|Salary|\n+---+-----+------+------+\n|  1|vinay|     M| 50000|\n|  2|kumar|     M| 43000|\n|  3|kavya|     F| 69000|\n+---+-----+------+------+\n\n+---+-----+------+------+\n| Id| Name|Gender|Salary|\n+---+-----+------+------+\n|  1|vinay|     M| 50000|\n|  2|kumar|     M| 43000|\n+---+-----+------+------+\n\n+---+-----+------+------+\n| Id| Name|Gender|Salary|\n+---+-----+------+------+\n|  1|vinay|     M| 50000|\n+---+-----+------+------+\n\n+---+-----+------+------+\n| Id| Name|Gender|Salary|\n+---+-----+------+------+\n|  1|vinay|     M| 50000|\n|  2|kumar|     M| 43000|\n|  3|kavya|     F| 69000|\n+---+-----+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [(1,'vinay','M',50000),(2,'kumar','M',43000),(3,'kavya','F',69000)]\n",
    "sch = ['Id','Name','Gender','Salary']\n",
    "df = spark.createDataFrame(data,sch)\n",
    "df.show()\n",
    "\n",
    "\n",
    "#Use of the filter()\n",
    "df.filter(df.Gender == 'M').show()\n",
    "df.filter((df.Gender == 'M') & (df.Salary >= 45000)).show() #We can use miltipule conditions init!\n",
    "df.where(df.Name.like('k%') | (df.Salary==50000)).show() #In the same way we can use the where just like filter() function.. !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb6a86c6-9a7e-4035-b38e-399dd144c88d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Now we will make use of `distinct()` and `dropDuplicates()` in pyspark !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50183ae6-86da-4ccb-8d16-de3cf02c0086",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n| Id|  Name|Gender|Salary|\n+---+------+------+------+\n|  1| vinay|     M| 50000|\n|  2| kumar|     M| 43000|\n|  3| vinay|     F| 69000|\n|  4|vijaya|     F| 97000|\n|  3| kavya|     F| 67000|\n+---+------+------+------+\n\n+---+------+------+------+\n| Id|  Name|Gender|Salary|\n+---+------+------+------+\n|  1| vinay|     M| 50000|\n|  2| kumar|     M| 43000|\n|  3| vinay|     F| 69000|\n|  4|vijaya|     F| 97000|\n|  3| kavya|     F| 67000|\n+---+------+------+------+\n\n+---+------+------+------+\n| Id|  Name|Gender|Salary|\n+---+------+------+------+\n|  1| vinay|     M| 50000|\n|  2| kumar|     M| 43000|\n|  3| vinay|     F| 69000|\n|  4|vijaya|     F| 97000|\n|  3| kavya|     F| 67000|\n+---+------+------+------+\n\n+---+------+------+------+\n| Id|  Name|Gender|Salary|\n+---+------+------+------+\n|  1| vinay|     M| 50000|\n|  2| kumar|     M| 43000|\n|  3| vinay|     F| 69000|\n|  4|vijaya|     F| 97000|\n+---+------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [(1,'vinay','M',50000),(2,'kumar','M',43000),(3,'vinay','F',69000),(4,'vijaya','F',97000),(3,'kavya','F',67000)]\n",
    "sch = ['Id','Name','Gender','Salary']\n",
    "df = spark.createDataFrame(data,sch)\n",
    "df.show()#If u observe closely you can find some duplicates in the dataframe !\n",
    "\n",
    "# if u want to view the distinct data from the dataframe !\n",
    "df.distinct().show()#This will show the non duplicate dataframe ! but the drawback of using this function is that it works on the whole dataframe !\n",
    "\n",
    "#If u need to view the dataframe based on the distinct values of  a single column or milt_column !\n",
    "df.dropDuplicates().show()#This will just rows as same as the distinct() function ! until u provide a list of column's in the function !\n",
    "df.dropDuplicates(['Id']).show() #If u provide the column name then it will only type the dropDuplicates in that column !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3ce0f2e-a9e1-4160-bd67-3c2d45927dcd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Now we will know the `orderBy()` and `sort()` in the pyspark !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87e1b5fa-6492-4c3b-b493-8a1d2dd73382",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+-----------+\n| Id|  Name|Gender|Salary|Departments|\n+---+------+------+------+-----------+\n|  1| vinay|     M| 50000|         HR|\n|  2| kumar|     M| 43000|  Marketing|\n|  3| kavya|     F| 69000|     DevOps|\n|  4|vijaya|     F| 97000|    Testing|\n|  5| supna|     F| 67800|         HR|\n+---+------+------+------+-----------+\n\n+---+------+------+------+-----------+\n| Id|  Name|Gender|Salary|Departments|\n+---+------+------+------+-----------+\n|  2| kumar|     M| 43000|  Marketing|\n|  1| vinay|     M| 50000|         HR|\n|  5| supna|     F| 67800|         HR|\n|  3| kavya|     F| 69000|     DevOps|\n|  4|vijaya|     F| 97000|    Testing|\n+---+------+------+------+-----------+\n\n+---+------+------+------+-----------+\n| Id|  Name|Gender|Salary|Departments|\n+---+------+------+------+-----------+\n|  4|vijaya|     F| 97000|    Testing|\n|  2| kumar|     M| 43000|  Marketing|\n|  1| vinay|     M| 50000|         HR|\n|  5| supna|     F| 67800|         HR|\n|  3| kavya|     F| 69000|     DevOps|\n+---+------+------+------+-----------+\n\n+---+------+------+------+-----------+\n| Id|  Name|Gender|Salary|Departments|\n+---+------+------+------+-----------+\n|  5| supna|     F| 67800|         HR|\n|  3| kavya|     F| 69000|     DevOps|\n|  4|vijaya|     F| 97000|    Testing|\n|  2| kumar|     M| 43000|  Marketing|\n|  1| vinay|     M| 50000|         HR|\n+---+------+------+------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [(1,'vinay','M',50000,'HR'),(2,'kumar','M',43000,'Marketing'),(3,'kavya','F',69000,'DevOps'),(4,'vijaya','F',97000,'Testing'),(5,'supna','F',67800,'HR')]\n",
    "sch = ['Id','Name','Gender','Salary','Departments']\n",
    "df = spark.createDataFrame(data,sch)\n",
    "df.show()\n",
    "\n",
    "#Use of the .sort()\n",
    "#This is used to sort the data in the dataframe on the perticular column !\n",
    "df.sort(df.Salary).show()#This will sort the dataframe based on the 'Salary' column and and prints the entire dataframe \n",
    "#In the above code u can see by default it take the asc() order but if u want to sort the dataframe based on the two columns and also first in asc() and second one is desc() !\n",
    "df.sort(df.Departments.desc(),df.Id).show()\n",
    "\n",
    "#Use of the orderBy() function\n",
    "df.orderBy(df.Gender,df.Salary).show() #This is just like the .sort() function !\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e3c9d2b-2e25-4a57-88bb-a44ad09f71af",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Now we will work on the `union()` and `unionAll()` functions in pyspark !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d23c9f47-eb5d-4719-aa17-b4e70b7433e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+-----------+\n| Id|  Name|Gender|Salary|Departments|\n+---+------+------+------+-----------+\n|  1| vinay|     M| 50000|         HR|\n|  2| kumar|     M| 43000|  Marketing|\n|  4|vijaya|     F| 97000|    Testing|\n+---+------+------+------+-----------+\n\n+---+------+------+------+-----------+\n| Id|  Name|Gender|Salary|Departments|\n+---+------+------+------+-----------+\n|  3| kavya|     F| 69000|     DevOps|\n|  4|vijaya|     F| 97000|    Testing|\n|  5| supna|     F| 67800|         HR|\n+---+------+------+------+-----------+\n\n+---+------+------+------+-----------+\n| Id|  Name|Gender|Salary|Departments|\n+---+------+------+------+-----------+\n|  1| vinay|     M| 50000|         HR|\n|  2| kumar|     M| 43000|  Marketing|\n|  4|vijaya|     F| 97000|    Testing|\n|  1| vinay|     M| 50000|         HR|\n|  2| kumar|     M| 43000|  Marketing|\n|  4|vijaya|     F| 97000|    Testing|\n+---+------+------+------+-----------+\n\n+---+------+------+------+-----------+\n| Id|  Name|Gender|Salary|Departments|\n+---+------+------+------+-----------+\n|  1| vinay|     M| 50000|         HR|\n|  2| kumar|     M| 43000|  Marketing|\n|  3| kavya|     F| 69000|     DevOps|\n|  4|vijaya|     F| 97000|    Testing|\n|  5| supna|     F| 67800|         HR|\n+---+------+------+------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [(1,'vinay','M',50000,'HR'),(2,'kumar','M',43000,'Marketing'),(4,'vijaya','F',97000,'Testing')]\n",
    "sch = ['Id','Name','Gender','Salary','Departments']\n",
    "df_1 = spark.createDataFrame(data,sch)\n",
    "df_1.show()\n",
    "data = [(3,'kavya','F',69000,'DevOps'),(4,'vijaya','F',97000,'Testing'),(5,'supna','F',67800,'HR')]\n",
    "sch = ['Id','Name','Gender','Salary','Departments']\n",
    "df_2 = spark.createDataFrame(data,sch)\n",
    "df_2.show()\n",
    "\n",
    "#This are the two different dataframes but both have the same schema!!\n",
    "#Use of union()\n",
    "df_1.union(df_1).show()\n",
    "#In the output you can find the duplicates because the .union() and unionAll() won't remove the duplicates !\n",
    "\n",
    "#Use of unionAll() same as the union()\n",
    "#If u want a dataframe that is clean on duplicates you can use the distinct() function with this !\n",
    "df_1.unionAll(df_2).distinct().sort(df_1.Id).show() #Now it is sorted and free of duplicates !!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee19a9fb-6647-46b3-b7cb-9c5b87ba4acb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Now we will work with `groupBy()` in pyspark !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d79667ca-9a4e-4246-9e9a-1349d512635b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------+------+-------+\n| id|     name|gender|salary|    dep|\n+---+---------+------+------+-------+\n|  1|   maheer|     M|  5000|     IT|\n|  2|     wafa|     M|  6000|     IT|\n|  3|      asi|     F|  2500|Payroll|\n|  4| sarfaraj|     M|  4000|     HR|\n|  5|pyarijaan|     F|  2000|     HR|\n|  6| Mahaboob|     M|  2000|Payroll|\n|  7|   ayesha|     F|  3000|     IT|\n+---+---------+------+------+-------+\n\n+------+-----+\n|gender|count|\n+------+-----+\n|     M|    4|\n|     F|    3|\n+------+-----+\n\n+-------+------+-----+\n|    dep|gender|count|\n+-------+------+-----+\n|     IT|     M|    2|\n|Payroll|     F|    1|\n|     HR|     M|    1|\n|     HR|     F|    1|\n|Payroll|     M|    1|\n|     IT|     F|    1|\n+-------+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'maheer', 'M' ,5000, 'IT'),\\\n",
    "          (2, 'wafa', 'M' ,6000, 'IT'),\\\n",
    "          (3, 'asi', 'F' ,2500, 'Payroll'),\\\n",
    "          (4, 'sarfaraj','M' ,4000, 'HR'),\\\n",
    "          (5, 'pyarijaan', 'F' ,2000, 'HR'),\\\n",
    "          (6, 'Mahaboob', 'M' ,2000, 'Payroll'),\\\n",
    "          (7, 'ayesha', 'F' ,3000, 'IT') ]\n",
    "schema = ('id', 'name', 'gender', 'salary','dep')\n",
    "df = spark.createDataFrame(data,schema)\n",
    "df.show()\n",
    "\n",
    "#Use of the .groupBy()\n",
    "df.groupBy(df.gender).count().show()\n",
    "\n",
    "#There are much more parameters on the groupBy() !!!\n",
    "df.select(df.columns).filter(df.salary >= 2000).sort(df.id.desc()).groupBy('dep','gender').count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1991bea-9505-4c3a-8d7f-2909ce54b962",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##No we will use groupBy `agg()` in pyspark !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "383b1675-9531-4e00-b6a4-02d9e49e4df2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "            NOTE:---->\n",
    "            \n",
    "            The groupBy() is an important function while groupin the data in the dataframe -->\n",
    "            the groupby() function always works with a atleast one aggraigate function like min(),max(),avg(),count(),sun(),etc...\n",
    "            but if u want to see the full power of the groupBy() function then you have to use the groupBy() function with .agg()\n",
    "            the .agg() function helps you to use more than one aggriatte functions witht he groupBy() function on the dataframe !\n",
    "            the syntax- works like this :-\n",
    "                            <data_frame>.groupBy(<column_name> --> that you want agroup with !)\\\n",
    "                                                            .agg(< \\\n",
    "                                                                aggriagat function {column_name} >,\\\n",
    "                                                                <{ More aggrigate functions }>).show()\n",
    "\n",
    "            thhe amazing thing about this groupBy() function is that if u ever wana store the list of column data which is grouped tot he column_ in the dataframe \n",
    "            for exaple i need to group the dataframe based on the \"dipartment\" and i need a new column that can store the list of names of the employes in that perticular\n",
    "            dipartment then you can simply use the collect_list(<column_name>)  in this case name of th employ ! and also you can sore the unique \n",
    "            employ names if there are duplicates in that names you can use collect_set(<column_name>) this will remove the duplicates ! from that list !\n",
    "\n",
    "            And there is other functions like first() and last() this will give the first value in the list of employ names \n",
    "            and last name of the employ in that list and also u can use this on some other column as well to see the first and last values of the grouped data !\n",
    "\n",
    "            And there is last aggricate function tha is super good ! to know \n",
    "            if you want to compare the data in the databframe based on the two columns where the first column is used to group the data in the dataframe \n",
    "            and the second column is used to show the distinct data in that column at the top and the aggricate function out put in the below the second column{distinct_data}\n",
    "            and besid the grouped column \n",
    "\n",
    "          The syntax- works like this :-\n",
    "                            <dataFrame>.groupBy(<Column_name>).pivot(<Column_name>).agg(<{some aggricate function !}>).show()\n",
    "                            this kind of aggricate function is used when you want to see the max('salary') in each 'gender' with grouped on the 'dipartment columns !'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47e612d0-926f-4132-a5c5-aa8d98dbdaf5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------+------+-------+\n| id|     name|gender|salary|    dep|\n+---+---------+------+------+-------+\n|  1|   maheer|     M|  5000|     IT|\n|  2|     wafa|     M|  6000|     IT|\n|  3|      asi|     F|  2500|Payroll|\n|  4| sarfaraj|     M|  4000|     HR|\n|  5|pyarijaan|     F|  2000|     HR|\n|  6| Mahaboob|     M|  2000|Payroll|\n|  7|   ayesha|     F|  3000|     IT|\n+---+---------+------+------+-------+\n\n+-------+---------+-----------+-----------+----------------------+-----------+----------+\n|dep    |No_Of_Emp|Mimimum_Sal|Maximum_Sal|Emp_Names             |first_value|last_value|\n+-------+---------+-----------+-----------+----------------------+-----------+----------+\n|IT     |3        |3000       |6000       |[maheer, wafa, ayesha]|maheer     |ayesha    |\n|Payroll|2        |2000       |2500       |[asi, Mahaboob]       |asi        |Mahaboob  |\n|HR     |2        |2000       |4000       |[sarfaraj, pyarijaan] |sarfaraj   |pyarijaan |\n+-------+---------+-----------+-----------+----------------------+-----------+----------+\n\n+-------+----+-----+\n|    dep|   F|    M|\n+-------+----+-----+\n|     HR|2000| 4000|\n|Payroll|2500| 2000|\n|     IT|3000|11000|\n+-------+----+-----+\n\n+-------+-------------+-------------+-------------+-------------+\n|    dep|F_sum(salary)|F_max(salary)|M_sum(salary)|M_max(salary)|\n+-------+-------------+-------------+-------------+-------------+\n|     HR|         2000|         2000|         4000|         4000|\n|Payroll|         2500|         2500|         2000|         2000|\n|     IT|         3000|         3000|        11000|         6000|\n+-------+-------------+-------------+-------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "data = [(1, 'maheer', 'M' ,5000, 'IT'),\\\n",
    "          (2, 'wafa', 'M' ,6000, 'IT'),\\\n",
    "          (3, 'asi', 'F' ,2500, 'Payroll'),\\\n",
    "          (4, 'sarfaraj','M' ,4000, 'HR'),\\\n",
    "          (5, 'pyarijaan', 'F' ,2000, 'HR'),\\\n",
    "          (6, 'Mahaboob', 'M' ,2000, 'Payroll'),\\\n",
    "          (7, 'ayesha', 'F' ,3000, 'IT') ]\n",
    "schema = ('id', 'name', 'gender', 'salary','dep')\n",
    "df = spark.createDataFrame(data,schema)\n",
    "df.show()\n",
    "df.groupBy('dep').agg(count('*').alias('No_Of_Emp')\\\n",
    "                                ,min('salary').alias('Mimimum_Sal'),\\\n",
    "                                max('salary').alias('Maximum_Sal'),\\\n",
    "                                collect_list('name').alias('Emp_Names'),\\\n",
    "                                first('name').alias('first_value'),\\\n",
    "                                last('name').alias('last_value')).show(truncate = False)\n",
    "df.groupBy('dep').pivot('gender').agg(sum('salary')).show()\n",
    "df.groupBy('dep').pivot('gender').agg(sum('salary'),max('salary')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef1fc551-c9c2-4e82-bba5-da9f1e0d3e46",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Now u will know about the `unionByName()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "363c134d-b9ae-4042-b503-6706b848b426",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+-----------+\n| Id|  Name|Gender|Salary|Departments|\n+---+------+------+------+-----------+\n|  1| vinay|     M| 50000|         HR|\n|  2| kumar|     M| 43000|  Marketing|\n|  4|vijaya|     F| 97000|    Testing|\n+---+------+------+------+-----------+\n\n+---+------+------+-----------+\n| Id|  Name|Gender|Departments|\n+---+------+------+-----------+\n|  3| kavya|     F|     DevOps|\n|  4|vijaya|     F|    Testing|\n|  5| supna|     F|         HR|\n+---+------+------+-----------+\n\n+---+------+------+------+-----------+\n| Id|  Name|Gender|Salary|Departments|\n+---+------+------+------+-----------+\n|  1| vinay|     M| 50000|         HR|\n|  2| kumar|     M| 43000|  Marketing|\n|  4|vijaya|     F| 97000|    Testing|\n|  3| kavya|     F|  null|     DevOps|\n|  4|vijaya|     F|  null|    Testing|\n|  5| supna|     F|  null|         HR|\n+---+------+------+------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "#The unionByName() function is used to merge two dataFrames where there schema's are different! by passing allowMissingColumns() value as \"True\"\n",
    "data = [(1,'vinay','M',50000,'HR'),(2,'kumar','M',43000,'Marketing'),(4,'vijaya','F',97000,'Testing')]\n",
    "sch = ['Id','Name','Gender','Salary','Departments']\n",
    "df_1 = spark.createDataFrame(data,sch)\n",
    "df_1.show()\n",
    "data = [(3,'kavya','F','DevOps'),(4,'vijaya','F','Testing'),(5,'supna','F','HR')]\n",
    "sch = ['Id','Name','Gender','Departments']\n",
    "df_2 = spark.createDataFrame(data,sch)\n",
    "df_2.show()\n",
    "\n",
    "\n",
    "# df_1.unionByName(df_2).show() #If u do this it will show the error ! like this {\" Cannot resolve column name \"Salary\" among (Id, Name, Gender, Departments). \"}\n",
    "df_1.unionByName(df_2,allowMissingColumns=True).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1d9f504-4e20-4bd3-8755-4e32177ddac4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##üç•Now we will know the use of `select()` function in pyspark !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc750c2f-bea2-4f0b-aead-48862d0828a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n| Id|  Name|Gender|Salary|\n+---+------+------+------+\n|  1| vinay|     M| 50000|\n|  2| kumar|     M| 43000|\n|  3| vinay|     F| 69000|\n|  4|vijaya|     F| 97000|\n|  3| kavya|     F| 67000|\n+---+------+------+------+\n\nroot\n |-- Id: long (nullable = true)\n |-- Name: string (nullable = true)\n |-- Gender: string (nullable = true)\n |-- Salary: long (nullable = true)\n\n+---+------+------+------+\n| Id|  Name|Gender|Salary|\n+---+------+------+------+\n|  1| vinay|     M| 50000|\n|  2| kumar|     M| 43000|\n|  3| vinay|     F| 69000|\n|  4|vijaya|     F| 97000|\n|  3| kavya|     F| 67000|\n+---+------+------+------+\n\n+---+------+------+------+\n| Id|  Name|Gender|Salary|\n+---+------+------+------+\n|  1| vinay|     M| 50000|\n|  2| kumar|     M| 43000|\n|  3| vinay|     F| 69000|\n|  4|vijaya|     F| 97000|\n|  3| kavya|     F| 67000|\n+---+------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "data = [(1,'vinay','M',50000),(2,'kumar','M',43000),(3,'vinay','F',69000),(4,'vijaya','F',97000),(3,'kavya','F',67000)]\n",
    "sch = ['Id','Name','Gender','Salary']\n",
    "\n",
    "df = spark.createDataFrame(data,sch)\n",
    "df.show()\n",
    "df.printSchema()\n",
    "df = df.withColumn('Id',col('Id').cast('Int')).withColumn('Salary',col('Salary').cast('Long'))#Used to cange the datatype !\n",
    "df.select([i for i in df.columns]).show()\n",
    "df.select('*').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b11a4e97-df58-418a-b14e-1aaac27654eb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##ü•êNow we use `join` in the pyspark !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35c4fb32-19d3-4b06-88a3-0e09c9377ba3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+\n| Id|  name|salary|dep|\n+---+------+------+---+\n|  1|maheer|  2008|  2|\n|  2|  wafa|  3000|  1|\n|  3|  abcd|  1000|  4|\n+---+------+------+---+\n\n+---+-------+\n| Id|   name|\n+---+-------+\n|  1|     IT|\n|  2|     HR|\n|  3|Payroll|\n+---+-------+\n\n+---+------+------+---+---+----+\n| Id|  name|salary|dep| Id|name|\n+---+------+------+---+---+----+\n|  2|  wafa|  3000|  1|  1|  IT|\n|  1|maheer|  2008|  2|  2|  HR|\n+---+------+------+---+---+----+\n\n+---+------+------+---+----+----+\n| Id|  name|salary|dep|  Id|name|\n+---+------+------+---+----+----+\n|  1|maheer|  2008|  2|   2|  HR|\n|  2|  wafa|  3000|  1|   1|  IT|\n|  3|  abcd|  1000|  4|null|null|\n+---+------+------+---+----+----+\n\n+----+------+------+----+---+-------+\n|  Id|  name|salary| dep| Id|   name|\n+----+------+------+----+---+-------+\n|   2|  wafa|  3000|   1|  1|     IT|\n|   1|maheer|  2008|   2|  2|     HR|\n|null|  null|  null|null|  3|Payroll|\n+----+------+------+----+---+-------+\n\n+----+------+------+----+----+-------+\n|  Id|  name|salary| dep|  Id|   name|\n+----+------+------+----+----+-------+\n|   2|  wafa|  3000|   1|   1|     IT|\n|   1|maheer|  2008|   2|   2|     HR|\n|null|  null|  null|null|   3|Payroll|\n|   3|  abcd|  1000|   4|null|   null|\n+----+------+------+----+----+-------+\n\n+---+------+------+---+\n| Id|  name|salary|dep|\n+---+------+------+---+\n|  2|  wafa|  3000|  1|\n|  1|maheer|  2008|  2|\n+---+------+------+---+\n\n+---+----+------+---+\n| Id|name|salary|dep|\n+---+----+------+---+\n|  3|abcd|  1000|  4|\n+---+----+------+---+\n\n+---+------+------+---+---+------+------+---+\n| Id|  name|salary|dep| Id|  name|salary|dep|\n+---+------+------+---+---+------+------+---+\n|  1|maheer|  2008|  2|  2|  wafa|  3000|  1|\n|  2|  wafa|  3000|  1|  1|maheer|  2008|  2|\n+---+------+------+---+---+------+------+---+\n\n"
     ]
    }
   ],
   "source": [
    "datal = [(1, 'maheer' ,2008,2),(2, 'wafa' ,3000,1),(3, 'abcd' ,1000,4)]\n",
    "schemal = ['Id', 'name', 'salary','dep']\n",
    "data2 = [(1,'IT'), (2, 'HR'),(3, 'Payroll')]\n",
    "schema2 = ['Id', 'name']\n",
    "df_1 = spark.createDataFrame (datal, schemal)\n",
    "df_2 = spark.createDataFrame (data2, schema2)\n",
    "df_1.show()\n",
    "df_2.show()\n",
    "\n",
    "\n",
    "#Now we will use join's !\n",
    "df_1.join(df_2,df_1.dep == df_2.Id,'Inner').show()#This is the inner join that we can see the matching rows of data in the both the #tables !\n",
    "df_1.join(df_2,df_1.dep == df_2.Id,'Left').show()#In this the df_1 is the left table and the df_2 is the right and we are using the \n",
    "#Left_join so the we will get all the data from the left table and matching data from the right and remaining it will take as null's!\n",
    "df_1.join(df_2,df_1.dep==df_2.Id,'Right').show()#This a right join !\n",
    "df_1.join(df_2,df_1.dep==df_2.Id,'Full').show()#This is a full join or you can say it is a combinaction on right and left join's!\n",
    "\n",
    "\n",
    "df_1.join(df_2,df_1.dep == df_2.Id,'Leftsemi').show()#The 'Leftsemi' is just like inner it will only shown the connected data rows\n",
    "#But the difference is that in leftsemi it will only the matching left table only ! in the output !\n",
    "\n",
    "df_1.join(df_2,df_1.dep == df_2.Id,'leftanti').show()#This is a the opposite to the leftsemi it will show the non matched rows of #the left table !\n",
    "#And there are no rightsemi or rightanti !\n",
    "\n",
    "df_1.alias('Emp_Id').join(df_1.alias('Mag_Id'),col('Emp_Id.Id') == col('Mag_Id.dep'),'inner').show()\n",
    "#Basically the self is not there in the pyspark but we can implement it in the above way !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de8c32c1-3d39-4c7a-a640-4494e9dbddec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##ü•§In this we will try to `unpivot` the dataframe that has been pivoted !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31008b7b-459d-4030-9caf-a94cc9e20b52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+------+\n|      DEP|MALE|FEMALE|\n+---------+----+------+\n|       IT|   5|     7|\n|       HR|   3|     7|\n|Marketing|   7|    11|\n+---------+----+------+\n\nroot\n |-- DEP: string (nullable = true)\n |-- MALE: string (nullable = true)\n |-- FEMALE: string (nullable = true)\n\nroot\n |-- DEP: string (nullable = true)\n |-- GENDERS: string (nullable = true)\n |-- COUNT: string (nullable = true)\n\n+---------+-------+-----+\n|      DEP|GENDERS|COUNT|\n+---------+-------+-----+\n|       IT|      M|    5|\n|       IT|      F|    7|\n|       HR|      M|    3|\n|       HR|      F|    7|\n|Marketing|      M|    7|\n|Marketing|      F|   11|\n+---------+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "#We know that what is pivot() and we have used this with the groupBy() !\n",
    "data = [('IT',5,7),('HR',3,7),('Marketing',7,11)]\n",
    "sch = ['DEP','MALE','FEMALE']\n",
    "df = spark.createDataFrame(data,sch)\n",
    "df = df.withColumn('MALE',col('MALE').cast('String')).withColumn('FEMALE',col('FEMALE').cast('String'))\n",
    "df.show()\n",
    "df.printSchema()\n",
    "#Now i will try to change the dataframe to unpivot dataframe !\n",
    "up_df = df.select('DEP',expr(\"stack(2,'M',MALE,'F',FEMALE)as(GENDERS,COUNT)\"))\n",
    "up_df.printSchema()\n",
    "up_df.show()#This is the unpivoted dataframe \n",
    "#In this i had to change the dattype of the mail and female \n",
    "#And also in this i have used the expr() that i sused to perform a spark function that is given in a full string ! and 2 is the \n",
    "#no of arguments and the 'M' is the data and MALE this is the column ! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a428178f-bdcc-4380-9099-19949a747b01",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##üåµNow we will use `fill()` and `fillna()` in the pyspark !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19a8a60a-b9c2-4940-9900-f03a8b4805ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+-----------+\n| Id|  Name|Gender|Salary|Departments|\n+---+------+------+------+-----------+\n|  1| vinay|     M| 50000|       null|\n|  2| kumar|     M|  null|  Marketing|\n|  4|vijaya|  null| 97000|    Testing|\n+---+------+------+------+-----------+\n\n+---+------+-------+------+-----------+\n| Id|  Name| Gender|Salary|Departments|\n+---+------+-------+------+-----------+\n|  1| vinay|      M| 50000|    UNKNOWN|\n|  2| kumar|      M|  null|  Marketing|\n|  4|vijaya|UNKNOWN| 97000|    Testing|\n+---+------+-------+------+-----------+\n\n+---+------+------+------+-----------+\n| Id|  Name|Gender|Salary|Departments|\n+---+------+------+------+-----------+\n|  1| vinay|     M| 50000|       null|\n|  2| kumar|     M|  null|  Marketing|\n|  4|vijaya| 00000| 97000|    Testing|\n+---+------+------+------+-----------+\n\n+---+------+------+------+-----------+\n| Id|  Name|Gender|Salary|Departments|\n+---+------+------+------+-----------+\n|  1| vinay|     M| 50000|       null|\n|  2| kumar|     M|     0|  Marketing|\n|  4|vijaya|  null| 97000|    Testing|\n+---+------+------+------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "#This fill() and fillna() is used to replace NULL/NONE values on all selected multiple\n",
    "#Dataframe columns with either zero(0), empty\n",
    "#String, space or any constant literal values !\n",
    "data = [(1,'vinay','M',50000,None),(2,'kumar','M',None,'Marketing'),(4,'vijaya',None,97000,'Testing')]\n",
    "sch = ['Id','Name','Gender','Salary','Departments']\n",
    "df_1 = spark.createDataFrame(data,sch)\n",
    "df_1.show()\n",
    "\n",
    "df_1.fillna('UNKNOWN').show()#This will make all the null values into 'UNKNOWN'#But it will remove the null value for string datatype columns only !\n",
    "df_1.fillna('00000',['Gender']).show()#This will only make the given column null values into '0000'\n",
    "df_1.fillna(0).show()#This will remove the null values from the Integer datatype columns ! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73c574e0-097d-4020-9b39-06947f6dad68",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##üõ∞Ô∏èNow we will use `sample()` in pyspark !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcbe6d50-a954-4ccf-82fd-ed266760dab7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n+---+------+-----------------+------+-------+-----+-----------------------------------------+\n|age|gender|name             |course|roll   |marks|email                                    |\n+---+------+-----------------+------+-------+-----+-----------------------------------------+\n|29 |Female|Jenna Montague   |MVC   |502585 |43   |Hubert Oliveras_Jc Andrepont@DSA.com     |\n|28 |Female|Somer Stoecker   |Cloud |612490 |82   |Sebrina Maresca_Gonzalo Ferebee@DSA.com  |\n|28 |Male  |Mitzi Seldon     |DSA   |622981 |45   |Lawanda Wohlwend_Loris Crossett@DSA.com  |\n|28 |Female|Sheryll Towler   |DSA   |871967 |64   |Michelle Ruggiero_Billi Clore@Cloud.com  |\n|29 |Male  |Clementina Menke |DB    |882200 |76   |Michelle Ruggiero_Jenna Montague@MVC.com |\n|29 |Male  |Naoma Fritts     |DB    |931295 |79   |Hubert Oliveras_Sheryll Towler@DSA.com   |\n|29 |Female|Eda Neathery     |Cloud |1011971|91   |Margene Moores_Elenore Choy@MVC.com      |\n|29 |Female|Hubert Oliveras  |DSA   |1041483|28   |Billi Clore_Santa Kerfien@DSA.com        |\n|29 |Male  |Donna Yerby      |DSA   |1092702|79   |Marylee Capasso_Alberta Freund@OOP.com   |\n|29 |Female|Abram Nagao      |DSA   |1181007|57   |Anna Santos_Anna Santos@DSA.com          |\n|28 |Male  |Nicole Harwood   |DB    |1211495|28   |Melani Engberg_Jc Andrepont@Cloud.com    |\n|28 |Male  |Paris Hutton     |MVC   |1292279|26   |Paris Hutton_Eda Neathery@DSA.com        |\n|28 |Female|Nicole Harwood   |PF    |1441367|95   |Jc Andrepont_Anna Santos@DSA.com         |\n|28 |Female|Cordie Harnois   |MVC   |1501333|94   |Gonzalo Ferebee_Tijuana Kropf@OOP.com    |\n|29 |Female|Taryn Brownlee   |OOP   |1511213|72   |Cheri Kenney_Donna Yerby@DSA.com         |\n|29 |Female|Annika Hoffman   |OOP   |1551846|50   |Paris Hutton_Melani Engberg@DB.com       |\n|29 |Female|Michelle Ruggiero|DB    |1642509|45   |Hubert Oliveras_Marylee Capasso@DB.com   |\n|28 |Female|Neda Briski      |Cloud |1651303|74   |Ernest Rossbach_Donna Yerby@OOP.com      |\n|29 |Female|Jc Andrepont     |MVC   |1731652|74   |Hubert Oliveras_Michelle Ruggiero@DSA.com|\n|28 |Female|Melani Engberg   |Cloud |1872667|99   |Alberta Freund_Nicole Harwood@DB.com     |\n+---+------+-----------------+------+-------+-----+-----------------------------------------+\nonly showing top 20 rows\n\n+---+------+------------------+------+-------+-----+-----------------------------------------+\n|age|gender|name              |course|roll   |marks|email                                    |\n+---+------+------------------+------+-------+-----+-----------------------------------------+\n|28 |Female|Claude Panos      |Cloud |72409  |85   |Sheryll Towler_Alberta Freund@Cloud.com  |\n|28 |Female|Mickey Cortright  |DB    |192537 |62   |Ernest Rossbach_Marylee Capasso@Cloud.com|\n|28 |Female|Alberta Freund    |OOP   |251805 |83   |Annika Hoffman_Sheryll Towler@MVC.com    |\n|29 |Female|Paris Hutton      |DSA   |271472 |99   |Sheryll Towler_Alberta Freund@DSA.com    |\n|29 |Female|Cheri Kenney      |Cloud |281408 |43   |Annika Hoffman_Melani Engberg@Cloud.com  |\n|29 |Female|Claude Panos      |OOP   |542821 |28   |Hubert Oliveras_Priscila Tavernier@PF.com|\n|28 |Male  |Priscila Tavernier|PF    |642594 |27   |Marylee Capasso_Claude Panos@DSA.com     |\n|28 |Female|Celeste Lollis    |DSA   |731879 |76   |Hubert Oliveras_Gonzalo Ferebee@DSA.com  |\n|28 |Female|Hubert Oliveras   |DB    |771081 |79   |Kizzy Brenner_Dustin Feagins@MVC.com     |\n|28 |Female|Santa Kerfien     |Cloud |851620 |34   |Melani Engberg_Sheryll Towler@PF.com     |\n|29 |Female|Taryn Brownlee    |MVC   |861849 |39   |Billi Clore_Toshiko Hillyard@MVC.com     |\n|29 |Female|Mickey Cortright  |Cloud |1421542|71   |Latia Vanhoose_Kena Wild@OOP.com         |\n|29 |Female|Jeannetta Golden  |PF    |1581770|39   |Bonita Higuera_Santa Kerfien@OOP.com     |\n|29 |Male  |Celeste Lollis    |PF    |1861656|67   |Niki Klimek_Lawanda Wohlwend@DB.com      |\n|28 |Female|Melani Engberg    |Cloud |1872667|99   |Alberta Freund_Nicole Harwood@DB.com     |\n|28 |Female|Anna Santos       |PF    |1911870|92   |Sebrina Maresca_Clementina Menke@OOP.com |\n|28 |Female|Cheri Kenney      |Cloud |2281771|22   |Tamera Blakley_Abram Nagao@OOP.com       |\n|29 |Female|Bonita Higuera    |DSA   |2312783|77   |Melani Engberg_Mitzi Seldon@DSA.com      |\n|29 |Male  |Somer Stoecker    |PF    |2721652|36   |Tijuana Kropf_Michelle Ruggiero@Cloud.com|\n|28 |Male  |Loris Crossett    |OOP   |2852490|25   |Loris Crossett_Donna Yerby@DSA.com       |\n+---+------+------------------+------+-------+-----+-----------------------------------------+\nonly showing top 20 rows\n\n+---+------+------------------+------+------+-----+-----------------------------------------+\n|age|gender|name              |course|roll  |marks|email                                    |\n+---+------+------------------+------+------+-----+-----------------------------------------+\n|28 |Male  |Celeste Lollis    |PF    |21267 |45   |Jeannetta Golden_Jenna Montague@DSA.com  |\n|28 |Female|Latia Vanhoose    |DB    |122502|27   |Latia Vanhoose_Mitzi Seldon@OOP.com      |\n|29 |Female|Latia Vanhoose    |MVC   |132110|55   |Eda Neathery_Nicole Harwood@Cloud.com    |\n|29 |Female|Latia Vanhoose    |DB    |152159|27   |Claude Panos_Santa Kerfien@DB.com        |\n|29 |Male  |Annika Hoffman    |OOP   |171660|22   |Taryn Brownlee_Mitzi Seldon@MVC.com      |\n|29 |Female|Anna Santos       |Cloud |242254|68   |Jc Andrepont_Hubert Oliveras@Cloud.com   |\n|28 |Female|Maybell Duguay    |Cloud |261439|20   |Nicole Harwood_Judie Chipps@DB.com       |\n|29 |Female|Claude Panos      |Cloud |302130|59   |Sheryll Towler_Leontine Phillips@PF.com  |\n|28 |Female|Mickey Cortright  |DSA   |342003|44   |Mitzi Seldon_Jeannetta Golden@PF.com     |\n|28 |Male  |Hubert Oliveras   |OOP   |351719|63   |Lawanda Wohlwend_Abram Nagao@MVC.com     |\n|28 |Male  |Sebrina Maresca   |PF    |361316|62   |Nicole Harwood_Latia Vanhoose@DB.com     |\n|29 |Female|Jenna Montague    |MVC   |502585|43   |Hubert Oliveras_Jc Andrepont@DSA.com     |\n|29 |Male  |Celeste Lollis    |DSA   |562065|85   |Jc Andrepont_Melani Engberg@OOP.com      |\n|28 |Male  |Marylee Capasso   |OOP   |581756|52   |Jeannetta Golden_Loris Crossett@MVC.com  |\n|28 |Male  |Priscila Tavernier|PF    |642594|27   |Marylee Capasso_Claude Panos@DSA.com     |\n|28 |Female|Jenna Montague    |DSA   |691564|26   |Mitzi Seldon_Sebrina Maresca@Cloud.com   |\n|29 |Male  |Maybell Duguay    |PF    |701486|99   |Clementina Menke_Latia Vanhoose@Cloud.com|\n|29 |Male  |Jenna Montague    |MVC   |711153|45   |Marylee Capasso_Hubert Oliveras@DB.com   |\n|28 |Female|Hubert Oliveras   |DB    |771081|79   |Kizzy Brenner_Dustin Feagins@MVC.com     |\n|28 |Female|Santa Kerfien     |Cloud |851620|34   |Melani Engberg_Sheryll Towler@PF.com     |\n+---+------+------------------+------+------+-----+-----------------------------------------+\nonly showing top 20 rows\n\n+---+------+------------------+------+------+-----+-----------------------------------------+\n|age|gender|name              |course|roll  |marks|email                                    |\n+---+------+------------------+------+------+-----+-----------------------------------------+\n|28 |Male  |Celeste Lollis    |PF    |21267 |45   |Jeannetta Golden_Jenna Montague@DSA.com  |\n|28 |Female|Latia Vanhoose    |DB    |122502|27   |Latia Vanhoose_Mitzi Seldon@OOP.com      |\n|29 |Female|Latia Vanhoose    |MVC   |132110|55   |Eda Neathery_Nicole Harwood@Cloud.com    |\n|29 |Female|Latia Vanhoose    |DB    |152159|27   |Claude Panos_Santa Kerfien@DB.com        |\n|29 |Male  |Annika Hoffman    |OOP   |171660|22   |Taryn Brownlee_Mitzi Seldon@MVC.com      |\n|29 |Female|Anna Santos       |Cloud |242254|68   |Jc Andrepont_Hubert Oliveras@Cloud.com   |\n|28 |Female|Maybell Duguay    |Cloud |261439|20   |Nicole Harwood_Judie Chipps@DB.com       |\n|29 |Female|Claude Panos      |Cloud |302130|59   |Sheryll Towler_Leontine Phillips@PF.com  |\n|28 |Female|Mickey Cortright  |DSA   |342003|44   |Mitzi Seldon_Jeannetta Golden@PF.com     |\n|28 |Male  |Hubert Oliveras   |OOP   |351719|63   |Lawanda Wohlwend_Abram Nagao@MVC.com     |\n|28 |Male  |Sebrina Maresca   |PF    |361316|62   |Nicole Harwood_Latia Vanhoose@DB.com     |\n|29 |Female|Jenna Montague    |MVC   |502585|43   |Hubert Oliveras_Jc Andrepont@DSA.com     |\n|29 |Male  |Celeste Lollis    |DSA   |562065|85   |Jc Andrepont_Melani Engberg@OOP.com      |\n|28 |Male  |Marylee Capasso   |OOP   |581756|52   |Jeannetta Golden_Loris Crossett@MVC.com  |\n|28 |Male  |Priscila Tavernier|PF    |642594|27   |Marylee Capasso_Claude Panos@DSA.com     |\n|28 |Female|Jenna Montague    |DSA   |691564|26   |Mitzi Seldon_Sebrina Maresca@Cloud.com   |\n|29 |Male  |Maybell Duguay    |PF    |701486|99   |Clementina Menke_Latia Vanhoose@Cloud.com|\n|29 |Male  |Jenna Montague    |MVC   |711153|45   |Marylee Capasso_Hubert Oliveras@DB.com   |\n|28 |Female|Hubert Oliveras   |DB    |771081|79   |Kizzy Brenner_Dustin Feagins@MVC.com     |\n|28 |Female|Santa Kerfien     |Cloud |851620|34   |Melani Engberg_Sheryll Towler@PF.com     |\n+---+------+------------------+------+------+-----+-----------------------------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#This sample() function is useed when there is a large dataset aand u want to use a fraction of data in that large dataframe !\n",
    "#And in thsi it will get the data row's randomle from the dataframe !\n",
    "\n",
    "\n",
    "#Now lets use a large dataset i mean a .csv file !\n",
    "df = spark.read.csv('dbfs:/FileStore/StudentData.csv',header = True,inferSchema=True)#This dataset contains 1000 lines of data \n",
    "print(df.count())\n",
    "\n",
    "#Now we will make a two different data sets from the df!\n",
    "df_1 = df.sample(fraction = 0.1)#In this '0.1' means the 10% of the total dataset and the row data is also random too !\n",
    "df_1.show(truncate = False)#This kind of function is mostly used in the spark Mlib !\n",
    "df_2 = df.sample(fraction = 0.1)\n",
    "df_2.show(truncate = False)\n",
    "#We can see that those two data that are in the df_1 and df_2 is so randome every time i run that data in that will change \n",
    "#If u want to get a same data in both the dataframes every time then we can use somthing called as seed value !\n",
    "#if you give the seed value as same to the both dataframes then we will get the same data in those dataframes every time~\n",
    "\n",
    "df_3 = df.sample(fraction = 0.2,seed = 234)\n",
    "df_3.show(truncate = False)\n",
    "df_4 = df.sample(fraction = 0.2,seed = 234)\n",
    "df_4.show(truncate = False)\n",
    "\n",
    "#Now you can see every time i run the code i will get the same data in the df_3 and df_4 !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d390f7f-a25d-4837-89ac-c23996f9803c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##üõµNow we use the `collect()` in pyspark !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f273402c-dbb9-49a1-9c3c-8bc141468d92",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+-----------+\n| Id|  Name|Gender|Salary|Departments|\n+---+------+------+------+-----------+\n|  1| vinay|     M| 50000|       null|\n|  2| kumar|     M|  null|  Marketing|\n|  4|vijaya|  null| 97000|    Testing|\n+---+------+------+------+-----------+\n\nroot\n |-- Id: integer (nullable = true)\n |-- Name: string (nullable = true)\n |-- Gender: string (nullable = true)\n |-- Salary: integer (nullable = true)\n |-- Departments: string (nullable = true)\n\n[Row(Id=1, Name='vinay', Gender='M', Salary=50000, Departments=None), Row(Id=2, Name='kumar', Gender='M', Salary=None, Departments='Marketing'), Row(Id=4, Name='vijaya', Gender=None, Salary=97000, Departments='Testing')]\nvinay\nRow(Id=2, Name='kumar', Gender='M', Salary=None, Departments='Marketing')\nkumar\nkumar\n"
     ]
    }
   ],
   "source": [
    "#The collect() retrieves all the elements in a dataframe as an array of row type to the driver node.\n",
    "#It will return a data in array \n",
    "#This can only be used in the case of the small dataframes because in the matter of large dataframes to the node the it is complecated !\n",
    "\n",
    "data = [(1,'vinay','M',50000,None),(2,'kumar','M',None,'Marketing'),(4,'vijaya',None,97000,'Testing')]\n",
    "sch = ['Id','Name','Gender','Salary','Departments']\n",
    "df_1 = spark.createDataFrame(data,sch)\n",
    "df_1 = df_1.withColumn('Id',col('Id').cast('Int')).withColumn('Salary',col('Salary').cast('Int'))\n",
    "df_1.show()\n",
    "df_1.printSchema()\n",
    "\n",
    "df_2 = df_1.collect()\n",
    "print(df_2)\n",
    "print(df_2[0].Name)\n",
    "print(df_2[1])\n",
    "print(df_2[1][1])\n",
    "print(df_2[1].Name)\n",
    "#This is how we het to use the .collect() function !\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06430c20-e87d-418e-9ca0-aa1c13be2b70",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##üõ∫Now we know the `.transform()` function in pyspark !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c02caba2-2590-44b3-8844-344cb85fa865",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n| Id|  Name|Gender|Salary|\n+---+------+------+------+\n|  1| vinay|     M| 50000|\n|  2| kumar|     M| 43000|\n|  3| vinay|     F| 69000|\n|  4|vijaya|     F| 97000|\n|  3| kavya|     F| 67000|\n+---+------+------+------+\n\n+---+------+------+------+\n| Id|  Name|Gender|Salary|\n+---+------+------+------+\n|  1| VINAY|     M| 50000|\n|  2| KUMAR|     M| 43000|\n|  3| VINAY|     F| 69000|\n|  4|VIJAYA|     F| 97000|\n|  3| KAVYA|     F| 67000|\n+---+------+------+------+\n\n+---+------+------+------+\n| Id|  Name|Gender|Salary|\n+---+------+------+------+\n|  1| vinay|  Male| 50000|\n|  2| kumar|  Male| 43000|\n|  3| kavya|Female| 67000|\n|  4|vijaya|Female| 97000|\n+---+------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#Now we are using the .transform() function on the df data_frame !\n",
    "data = [(1,'vinay','M',50000),(2,'kumar','M',43000),(3,'vinay','F',69000),(4,'vijaya','F',97000),(3,'kavya','F',67000)]\n",
    "sch = ['Id','Name','Gender','Salary']\n",
    "df = spark.createDataFrame(data,sch)\n",
    "df.show()\n",
    "df.withColumn('Name',upper('Name')).show()#This is a transformation that we are using on the dataframe but it is a perdefined\n",
    "#Transformation so ... If i need to build a my custom transformation inn python and use that transformation on the dataframe \n",
    "#This can be done by using the .transform() function !\n",
    "\n",
    "def jj(df):\n",
    "    return df.withColumn('Gender',when(df.Gender == 'M','Male').otherwise('Female')).dropDuplicates(['Name']).orderBy('Id')\n",
    "df_1 = df.transform(jj)\n",
    "df_1.show()\n",
    "#This is a python function and i am using this function how many time i want to and to use this function wee need to use the \n",
    "#.transform() function and make use of this function !\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f67e312-2730-4dfc-97f9-204cb5de621f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##üõ©Ô∏èNow we know the differencte between `.transform()` function on dataFrames and `pyspark.sql.functions.transform()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baec3ca1-1b1c-4e9b-9866-027f4d39c2df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------------------------+\n|Id |Name  |Expertices                   |\n+---+------+-----------------------------+\n|1  |vinay |[Hr, DeOps, Frontend]        |\n|2  |kumar |[marketing, DeOps, sales]    |\n|3  |vinay |[Tester, fullstack, Backend] |\n|4  |vijaya|[Hr, cybersecurity, tester]  |\n|3  |kavya |[marketing, cloud, advertise]|\n+---+------+-----------------------------+\n\nroot\n |-- Id: long (nullable = true)\n |-- Name: string (nullable = true)\n |-- Expertices: array (nullable = true)\n |    |-- element: string (containsNull = true)\n\n+---+------+-----------------------------+\n|Id |Name  |Expertices                   |\n+---+------+-----------------------------+\n|1  |vinay |[HR, DEOPS, FRONTEND]        |\n|2  |kumar |[MARKETING, DEOPS, SALES]    |\n|3  |vinay |[TESTER, FULLSTACK, BACKEND] |\n|4  |vijaya|[HR, CYBERSECURITY, TESTER]  |\n|3  |kavya |[MARKETING, CLOUD, ADVERTISE]|\n+---+------+-----------------------------+\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4291357246911695>:18\u001B[0m\n",
       "\u001B[1;32m     15\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m     16\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDevOps\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
       "\u001B[0;32m---> 18\u001B[0m df_1 \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mId\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mName\u001B[39m\u001B[38;5;124m'\u001B[39m,transform(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mExpertices\u001B[39m\u001B[38;5;124m'\u001B[39m,ooo)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mExp\u001B[39m\u001B[38;5;124m'\u001B[39m))\n",
       "\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# df_1 = df.withColumn('Expertices', expr(\"transform(Expertices, x -> IF(x = 'DeOps', 'DevOps', x))\"))\u001B[39;00m\n",
       "\u001B[1;32m     22\u001B[0m df_1\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py:164\u001B[0m, in \u001B[0;36mtry_remote_functions.<locals>.wrapped\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    162\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(functions, f\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m    163\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 164\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/functions.py:9186\u001B[0m, in \u001B[0;36mtransform\u001B[0;34m(col, f)\u001B[0m\n",
       "\u001B[1;32m   9132\u001B[0m \u001B[38;5;129m@try_remote_functions\u001B[39m\n",
       "\u001B[1;32m   9133\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtransform\u001B[39m(\n",
       "\u001B[1;32m   9134\u001B[0m     col: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumnOrName\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m   9135\u001B[0m     f: Union[Callable[[Column], Column], Callable[[Column, Column], Column]],\n",
       "\u001B[1;32m   9136\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Column:\n",
       "\u001B[1;32m   9137\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   9138\u001B[0m \u001B[38;5;124;03m    Returns an array of elements after applying a transformation to each element in the input array.\u001B[39;00m\n",
       "\u001B[1;32m   9139\u001B[0m \n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   9184\u001B[0m \u001B[38;5;124;03m    +--------------+\u001B[39;00m\n",
       "\u001B[1;32m   9185\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 9186\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_invoke_higher_order_function\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mArrayTransform\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43mcol\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43mf\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/functions.py:9117\u001B[0m, in \u001B[0;36m_invoke_higher_order_function\u001B[0;34m(name, cols, funs)\u001B[0m\n",
       "\u001B[1;32m   9114\u001B[0m expr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(expressions, name)\n",
       "\u001B[1;32m   9116\u001B[0m jcols \u001B[38;5;241m=\u001B[39m [_to_java_column(col)\u001B[38;5;241m.\u001B[39mexpr() \u001B[38;5;28;01mfor\u001B[39;00m col \u001B[38;5;129;01min\u001B[39;00m cols]\n",
       "\u001B[0;32m-> 9117\u001B[0m jfuns \u001B[38;5;241m=\u001B[39m [_create_lambda(f) \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m funs]\n",
       "\u001B[1;32m   9119\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m Column(sc\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mColumn(expr(\u001B[38;5;241m*\u001B[39mjcols \u001B[38;5;241m+\u001B[39m jfuns)))\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/functions.py:9117\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n",
       "\u001B[1;32m   9114\u001B[0m expr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(expressions, name)\n",
       "\u001B[1;32m   9116\u001B[0m jcols \u001B[38;5;241m=\u001B[39m [_to_java_column(col)\u001B[38;5;241m.\u001B[39mexpr() \u001B[38;5;28;01mfor\u001B[39;00m col \u001B[38;5;129;01min\u001B[39;00m cols]\n",
       "\u001B[0;32m-> 9117\u001B[0m jfuns \u001B[38;5;241m=\u001B[39m [\u001B[43m_create_lambda\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m funs]\n",
       "\u001B[1;32m   9119\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m Column(sc\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mColumn(expr(\u001B[38;5;241m*\u001B[39mjcols \u001B[38;5;241m+\u001B[39m jfuns)))\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/functions.py:9081\u001B[0m, in \u001B[0;36m_create_lambda\u001B[0;34m(f)\u001B[0m\n",
       "\u001B[1;32m   9073\u001B[0m argnames \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mx\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mz\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
       "\u001B[1;32m   9074\u001B[0m args \u001B[38;5;241m=\u001B[39m [\n",
       "\u001B[1;32m   9075\u001B[0m     _unresolved_named_lambda_variable(\n",
       "\u001B[1;32m   9076\u001B[0m         expressions\u001B[38;5;241m.\u001B[39mUnresolvedNamedLambdaVariable\u001B[38;5;241m.\u001B[39mfreshVarName(arg)\n",
       "\u001B[1;32m   9077\u001B[0m     )\n",
       "\u001B[1;32m   9078\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m arg \u001B[38;5;129;01min\u001B[39;00m argnames[: \u001B[38;5;28mlen\u001B[39m(parameters)]\n",
       "\u001B[1;32m   9079\u001B[0m ]\n",
       "\u001B[0;32m-> 9081\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   9083\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(result, Column):\n",
       "\u001B[1;32m   9084\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkValueError(\n",
       "\u001B[1;32m   9085\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHIGHER_ORDER_FUNCTION_SHOULD_RETURN_COLUMN\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m   9086\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunc_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: f\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreturn_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(result)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n",
       "\u001B[1;32m   9087\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m<command-4291357246911695>:13\u001B[0m, in \u001B[0;36mooo\u001B[0;34m(x)\u001B[0m\n",
       "\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mooo\u001B[39m(x):\n",
       "\u001B[0;32m---> 13\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m  \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mDeOps\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m:\n",
       "\u001B[1;32m     14\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m upper(x)\n",
       "\u001B[1;32m     15\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: 'in <string>' requires string as left operand, not Column"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-4291357246911695>:18\u001B[0m\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     16\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDevOps\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m---> 18\u001B[0m df_1 \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mId\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mName\u001B[39m\u001B[38;5;124m'\u001B[39m,transform(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mExpertices\u001B[39m\u001B[38;5;124m'\u001B[39m,ooo)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mExp\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# df_1 = df.withColumn('Expertices', expr(\"transform(Expertices, x -> IF(x = 'DeOps', 'DevOps', x))\"))\u001B[39;00m\n\u001B[1;32m     22\u001B[0m df_1\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py:164\u001B[0m, in \u001B[0;36mtry_remote_functions.<locals>.wrapped\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    162\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(functions, f\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    163\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 164\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/functions.py:9186\u001B[0m, in \u001B[0;36mtransform\u001B[0;34m(col, f)\u001B[0m\n\u001B[1;32m   9132\u001B[0m \u001B[38;5;129m@try_remote_functions\u001B[39m\n\u001B[1;32m   9133\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtransform\u001B[39m(\n\u001B[1;32m   9134\u001B[0m     col: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumnOrName\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   9135\u001B[0m     f: Union[Callable[[Column], Column], Callable[[Column, Column], Column]],\n\u001B[1;32m   9136\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Column:\n\u001B[1;32m   9137\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   9138\u001B[0m \u001B[38;5;124;03m    Returns an array of elements after applying a transformation to each element in the input array.\u001B[39;00m\n\u001B[1;32m   9139\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   9184\u001B[0m \u001B[38;5;124;03m    +--------------+\u001B[39;00m\n\u001B[1;32m   9185\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 9186\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_invoke_higher_order_function\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mArrayTransform\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43mcol\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43mf\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/functions.py:9117\u001B[0m, in \u001B[0;36m_invoke_higher_order_function\u001B[0;34m(name, cols, funs)\u001B[0m\n\u001B[1;32m   9114\u001B[0m expr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(expressions, name)\n\u001B[1;32m   9116\u001B[0m jcols \u001B[38;5;241m=\u001B[39m [_to_java_column(col)\u001B[38;5;241m.\u001B[39mexpr() \u001B[38;5;28;01mfor\u001B[39;00m col \u001B[38;5;129;01min\u001B[39;00m cols]\n\u001B[0;32m-> 9117\u001B[0m jfuns \u001B[38;5;241m=\u001B[39m [_create_lambda(f) \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m funs]\n\u001B[1;32m   9119\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m Column(sc\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mColumn(expr(\u001B[38;5;241m*\u001B[39mjcols \u001B[38;5;241m+\u001B[39m jfuns)))\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/functions.py:9117\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m   9114\u001B[0m expr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(expressions, name)\n\u001B[1;32m   9116\u001B[0m jcols \u001B[38;5;241m=\u001B[39m [_to_java_column(col)\u001B[38;5;241m.\u001B[39mexpr() \u001B[38;5;28;01mfor\u001B[39;00m col \u001B[38;5;129;01min\u001B[39;00m cols]\n\u001B[0;32m-> 9117\u001B[0m jfuns \u001B[38;5;241m=\u001B[39m [\u001B[43m_create_lambda\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m funs]\n\u001B[1;32m   9119\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m Column(sc\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mColumn(expr(\u001B[38;5;241m*\u001B[39mjcols \u001B[38;5;241m+\u001B[39m jfuns)))\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/functions.py:9081\u001B[0m, in \u001B[0;36m_create_lambda\u001B[0;34m(f)\u001B[0m\n\u001B[1;32m   9073\u001B[0m argnames \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mx\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mz\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m   9074\u001B[0m args \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m   9075\u001B[0m     _unresolved_named_lambda_variable(\n\u001B[1;32m   9076\u001B[0m         expressions\u001B[38;5;241m.\u001B[39mUnresolvedNamedLambdaVariable\u001B[38;5;241m.\u001B[39mfreshVarName(arg)\n\u001B[1;32m   9077\u001B[0m     )\n\u001B[1;32m   9078\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m arg \u001B[38;5;129;01min\u001B[39;00m argnames[: \u001B[38;5;28mlen\u001B[39m(parameters)]\n\u001B[1;32m   9079\u001B[0m ]\n\u001B[0;32m-> 9081\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   9083\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(result, Column):\n\u001B[1;32m   9084\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkValueError(\n\u001B[1;32m   9085\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHIGHER_ORDER_FUNCTION_SHOULD_RETURN_COLUMN\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   9086\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunc_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: f\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreturn_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(result)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n\u001B[1;32m   9087\u001B[0m     )\n\nFile \u001B[0;32m<command-4291357246911695>:13\u001B[0m, in \u001B[0;36mooo\u001B[0;34m(x)\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mooo\u001B[39m(x):\n\u001B[0;32m---> 13\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m  \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mDeOps\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m:\n\u001B[1;32m     14\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m upper(x)\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\n\u001B[0;31mTypeError\u001B[0m: 'in <string>' requires string as left operand, not Column",
       "errorSummary": "<span class='ansi-red-fg'>TypeError</span>: 'in <string>' requires string as left operand, not Column",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The 'pyspark.sql.functions.transform()' is is used to apply the transform on a column of Array-Type\n",
    "# This function applies to specific transformation on every element of the array and returns an object of ArrayType !\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "data = [(1,'vinay',['Hr','DeOps','Frontend']),(2,'kumar',['marketing','DeOps','sales']),(3,'vinay',['Tester','fullstack','Backend']),(4,'vijaya',['Hr','cybersecurity','tester']),(3,'kavya',['marketing','cloud','advertise'])]\n",
    "sch = ['Id','Name','Expertices']\n",
    "df = spark.createDataFrame(data,sch)\n",
    "df.show(truncate = False)\n",
    "df.printSchema()\n",
    "\n",
    "df.select('Id','Name',transform('Expertices',lambda x: upper(x)).alias('Expertices')).show(truncate = False)\n",
    "def ooo(x):\n",
    "    if x not in  'DeOps':\n",
    "        return upper(x)\n",
    "    else:\n",
    "        return 'DevOps'\n",
    "\n",
    "df_1 = df.select('Id','Name',transform('Expertices',ooo).alias('Exp'))\n",
    "\n",
    "\n",
    "# df_1 = df.withColumn('Expertices', expr(\"transform(Expertices, x -> IF(x = 'DeOps', 'DevOps', x))\"))\n",
    "df_1.show()\n",
    "\n",
    "\n",
    "#Need to learn somthing more ~~~~~~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f796ae63-3240-4178-ad9a-6393590f12aa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##ü™êNow we will use `createOrReplaceTempView()` function in pyspark !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4317c415-3f59-4605-8b60-b7706292c276",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n| Id|  Name|Gender|Salary|\n+---+------+------+------+\n|  1| vinay|     M| 50000|\n|  2| kumar|     M| 43000|\n|  3| vinay|     F| 69000|\n|  4|vijaya|     F| 97000|\n|  3| kavya|     F| 67000|\n+---+------+------+------+\n\n+------+\n|  Name|\n+------+\n| vinay|\n| kumar|\n|vijaya|\n| kavya|\n+------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#it will create a temporary view (Like a table ! to perform sql on that view !) on dataframe using this function!\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "data = [(1,'vinay','M',50000),(2,'kumar','M',43000),(3,'vinay','F',69000),(4,'vijaya','F',97000),(3,'kavya','F',67000)]\n",
    "sch = ['Id','Name','Gender','Salary']\n",
    "df = spark.createDataFrame(data,sch)\n",
    "df.show()\n",
    "df.createOrReplaceTempView('data')\n",
    "df_1 = spark.sql(\"SELECT DISTINCT(Name) FROM data\")\n",
    "df_1.show()\n",
    "#This is how we can use the `SQl` syntax on the created view !\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3f6fc22-fab8-4c27-9fc6-e44b0a92b4f6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##üßÉNow we will use `createOrReplaceGlobleTempView()` function in pyspark !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d452a0a5-009b-4edd-bf42-3dc5012674c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n|Domain   |\n+---------+\n|HR       |\n|MARKETING|\n|TESTER   |\n|HR       |\n|MARKETING|\n+---------+\n\nOut[102]: [Table(name='data', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n Table(name='zoro', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# The difference between this two  is that the createOrReplacetempView() function will create a view where we can perform the sql\n",
    "# Operation on that view or a table but we can only access that view in that session only !\n",
    "\n",
    "# But when it comes to the createOrReplaceGlobaltempView() function is used to create a view or a table of a dataframe and \n",
    "# where we can access that view in different sessions or in Notebooks !\n",
    "\n",
    "# All the created views will be in the 'global_temp' name space and we need to use this namespace while using that view !\n",
    "\n",
    "# Now we will create a Global view and to access that view and perform the operations on that !\n",
    "data = [(1,'vinay',['Hr','DeOps','Frontend']),(2,'kumar',['marketing','DeOps','sales']),(3,'vinay',['Tester','fullstack','Backend']),(4,'vijaya',['Hr','cybersecurity','tester']),(3,'kavya',['marketing','cloud','advertise'])]\n",
    "sch = ['Id','Name','Expertices']\n",
    "df = spark.createDataFrame(data,sch)\n",
    "\n",
    "df.createOrReplaceGlobalTempView('man')\n",
    "\n",
    "df_1 = spark.sql('SELECT  upper(Expertices[0])  as Domain FROM global_temp.man')# For this we will get the error if name_space is not used !\n",
    "df_1.show(truncate = False)\n",
    "# Now we can access this view in other session or in other notebook also !\n",
    "\n",
    "# If u need to view all the globale views then use \n",
    "spark.catalog.listTables('global_temp')\n",
    "\n",
    "# If u need to view all the local views then use \n",
    "spark.catalog.listTables('default')\n",
    "\n",
    "# If u every want to drop the views that u have created either it is local or a global view we can use \n",
    "# -------------> spark.catalog.dropTempView(<Local_view_name>)\n",
    "# -------------> spark.catalog.dropGlobalTempView(<Global_view_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21a3eca1-3153-4df7-9ba5-c2cf38afce65",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##ü•°Now we will use `udf()` function in pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e6e7ff6-9df6-4b01-b0e4-fec60fe205d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------------------------+\n|Id |Name  |Expertices                   |\n+---+------+-----------------------------+\n|1  |vinay |[Hr, DeOps, Frontend]        |\n|2  |kumar |[marketing, DeOps, sales]    |\n|3  |vinay |[Tester, fullstack, Backend] |\n|4  |vijaya|[Hr, cybersecurity, tester]  |\n|3  |kavya |[marketing, cloud, advertise]|\n+---+------+-----------------------------+\n\n+---+------+-----------------------------+\n|Id |Name  |Expertices                   |\n+---+------+-----------------------------+\n|1  |vinay |[Hr, DevOps, Frontend]       |\n|2  |kumar |[Marketing, DevOps, Sales]   |\n|3  |vinay |[Tester, Fullstack, Backend] |\n|4  |vijaya|[Hr, Cybersecurity, Tester]  |\n|3  |kavya |[Marketing, Cloud, Advertise]|\n+---+------+-----------------------------+\n\n+---+------+--------------------+\n| Id|  Name|          Expertices|\n+---+------+--------------------+\n|  1| Vinay|[Hr, DevOps, Fron...|\n|  2| Kumar|[Marketing, DevOp...|\n|  3| Vinay|[Tester, Fullstac...|\n|  4|Vijaya|[Hr, Cybersecurit...|\n|  3| Kavya|[Marketing, Cloud...|\n+---+------+--------------------+\n\n+---+------+--------------------+-----+------+\n| Id|  Name|          Expertices|Bonus|Salary|\n+---+------+--------------------+-----+------+\n|  1| Vinay|[Hr, DevOps, Fron...|45000| 96000|\n|  2| Kumar|[Marketing, DevOp...|45000| 96000|\n|  3| Vinay|[Tester, Fullstac...|45000| 96000|\n|  4|Vijaya|[Hr, Cybersecurit...|45000| 96000|\n|  3| Kavya|[Marketing, Cloud...|45000| 96000|\n+---+------+--------------------+-----+------+\n\n+---+------+-----------------------------+-----+------+---------+\n|Id |Name  |Expertices                   |Bonus|Salary|Total_Sal|\n+---+------+-----------------------------+-----+------+---------+\n|1  |Vinay |[Hr, DevOps, Frontend]       |45000|96000 |141000   |\n|2  |Kumar |[Marketing, DevOps, Sales]   |45000|96000 |141000   |\n|3  |Vinay |[Tester, Fullstack, Backend] |45000|96000 |141000   |\n|4  |Vijaya|[Hr, Cybersecurity, Tester]  |45000|96000 |141000   |\n|3  |Kavya |[Marketing, Cloud, Advertise]|45000|96000 |141000   |\n+---+------+-----------------------------+-----+------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# The udf() is nothing but the user_defined_functions() this are used to create a python logic in a function and use that function\n",
    "# on the dataframe or a column for the tranformation !\n",
    "\n",
    "\n",
    "\n",
    "data = [(1,'vinay',['Hr','DeOps','Frontend']),(2,'kumar',['marketing','DeOps','sales']),(3,'vinay',['Tester','fullstack','Backend']),(4,'vijaya',['Hr','cybersecurity','tester']),(3,'kavya',['marketing','cloud','advertise'])]\n",
    "sch = ['Id','Name','Expertices']\n",
    "df = spark.createDataFrame(data,sch)\n",
    "df.show(truncate = False)\n",
    "\n",
    "\n",
    "def kio(x):\n",
    "    k1 =[]\n",
    "    for i in x:\n",
    "        if i == 'DeOps':\n",
    "            k1.append('DevOps')\n",
    "        else:\n",
    "            k1.append(i.capitalize())\n",
    "    return k1\n",
    "\n",
    "kl=udf(kio,ArrayType(StringType()))\n",
    "df_1 = df.withColumn('Expertices',kl(col('Expertices')))\n",
    "df_1.show(truncate = False)\n",
    "# This is how you can create a transformation on the column in the dataframe and create a function\n",
    "# on the user defined transformation !\n",
    "\n",
    "def hhhh(c):#This is an other udf function !\n",
    "    return c.capitalize()\n",
    "cap = udf(hhhh,StringType())\n",
    "df_2 = df_1.withColumn('Name',cap(col('Name')))\n",
    "df_2.show()\n",
    "\n",
    "#lets create a 'bonus' named column !\n",
    "df_3 = df_2.withColumn('Bonus',lit(45000))\n",
    "\n",
    "\n",
    "#lets create a 'Salary' named column !\n",
    "df_3 = df_3.withColumn('Salary',lit(96000))\n",
    "df_3.show()\n",
    "\n",
    "# Now we will create a function that will take the both the columns data and sum's up the data in that and place's that\n",
    "# Data in a new column !\n",
    "\n",
    "# Now we will see a new way to use this udf() function using the annotations !\n",
    "\n",
    "# ----------------------------------------------IMP------------------------------------------------------------------\n",
    "\n",
    "@udf(returnType = IntegerType())# This a annotation way where we no need to create a name and assine the udf() function !\n",
    "def ghh(x,y):\n",
    "    return x+y\n",
    "\n",
    "\n",
    "df_4 = df_3.withColumn('Total_Sal',ghh(col('Bonus'),col('Salary')))\n",
    "df_4.show(truncate = False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb0afbfa-4d03-4f10-8fe3-67c9f7f9a417",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## üôÇCan we use this kind of udf() in sql also !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f37e976-6fa7-4dd1-880d-20f472c2738a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------------------------+\n|Id |Name  |EXP                      |\n+---+------+-------------------------+\n|1  |Vinay |Hr,DevOps,Frontend       |\n|2  |Kumar |Marketing,DevOps,Sales   |\n|3  |Vinay |Tester,Fullstack,Backend |\n|4  |Vijaya|Hr,Cybersecurity,Tester  |\n|3  |Kavya |Marketing,Cloud,Advertise|\n+---+------+-------------------------+\n\nroot\n |-- Id: long (nullable = true)\n |-- Name: string (nullable = true)\n |-- EXP: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# This peace of code is related to the upper_cell code !\n",
    "\n",
    "# Let's create a function that takes a ArrayType and prints out the string of all the elements !\n",
    "\n",
    "def qw(x):\n",
    "    return ','.join(i.strip() for i in x)\n",
    "\n",
    "# Now we have to register this function to be used in theh sql syntax !!!\n",
    "spark.udf.register(name ='Exp_Rolls',f = qw,returnType=StringType())\n",
    "# In this above register() part you will find three attributes 1) --> function name ( to use in SQL) \n",
    "# 2) --> f it is the actual function name that we have created !\n",
    "# 3) --> returnType() this is like what type of dataType the return data from the function !\n",
    "\n",
    "# Now to use this above created and registered function we need to create a temp_view of the dataFrame !\n",
    "df_4.createOrReplaceTempView('dev_data')\n",
    "\n",
    "df_5 = spark.sql('SELECT Id, Name,Exp_Rolls(Expertices) as EXP from dev_data')\n",
    "df_5.show(truncate = False)\n",
    "df_5.printSchema()\n",
    "\n",
    "# This is how we can perform the udf function on the sql also !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea65e6d1-de82-4138-98b6-8f67e1695960",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#  üòéFUN FACT\n",
    "          That you can send a whole dataframe to a user_defined_function udf() and expect the whole data frame as an output from the udf() function\n",
    "          while we are providing the dataframe we need to know the schema that datatype and we need to create a custom dataframe that we \n",
    "          we will get from the udf() function and we will give the schema name in the returnType = attribute !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7eb95801-8816-48a9-bfb5-1982ea6ef684",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##üç´Now we will see how we can convert an `rdd` object in the dataframe in pyspark !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c9fa388-4b16-4c3e-b763-974824ffe43c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'vinay', ['Hr', 'DeOps', 'Frontend']), (2, 'kumar', ['marketing', 'DeOps', 'sales']), (3, 'vinay', ['Tester', 'fullstack', 'Backend']), (4, 'vijaya', ['Hr', 'cybersecurity', 'tester']), (3, 'kavya', ['marketing', 'cloud', 'advertise'])]\n+---+------+-----------------------------+\n|Id |Name  |Expertices                   |\n+---+------+-----------------------------+\n|1  |vinay |[Hr, DeOps, Frontend]        |\n|2  |kumar |[marketing, DeOps, sales]    |\n|3  |vinay |[Tester, fullstack, Backend] |\n|4  |vijaya|[Hr, cybersecurity, tester]  |\n|3  |kavya |[marketing, cloud, advertise]|\n+---+------+-----------------------------+\n\n+---+------+-----------------------------+\n|Id |Name  |Expertices                   |\n+---+------+-----------------------------+\n|1  |vinay |[Hr, DeOps, Frontend]        |\n|2  |kumar |[marketing, DeOps, sales]    |\n|3  |vinay |[Tester, fullstack, Backend] |\n|4  |vijaya|[Hr, cybersecurity, tester]  |\n|3  |kavya |[marketing, cloud, advertise]|\n+---+------+-----------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# This the data \n",
    "data = [(1,'vinay',['Hr','DeOps','Frontend']),(2,'kumar',['marketing','DeOps','sales']),(3,'vinay',['Tester','fullstack','Backend']),(4,'vijaya',['Hr','cybersecurity','tester']),(3,'kavya',['marketing','cloud','advertise'])]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data) #The parallelize function will make a data into a rdd !\n",
    "print(rdd.collect())# Now we are viewing the data in a list formate !\n",
    "\n",
    "\n",
    "# Now we need to convert the rdd into the DataFrame !\n",
    "df = rdd.toDF(['Id','Name','Expertices'])\n",
    "df.show(truncate = False)\n",
    "# Now we have created the dataframe from the rdd !\n",
    "# we can use this function to make a dataframe from rdd\n",
    "\n",
    "df_1 = spark.createDataFrame(rdd,['Id','Name','Expertices'])\n",
    "df_1.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c555d26-bc90-4df5-9a43-fd42bc592cb7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##üé´Now we will use the `map()` transformation in pyspark !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1986cd0-cd29-4619-b9a2-3cd06a559bf1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('vinay', 'kumar'), ('sai', 'raj'), ('ajay', 'row')]\n[('vinaykumar',), ('sairaj',), ('ajayrow',)]\n"
     ]
    }
   ],
   "source": [
    "# The Map() transformation used to apply function(lambda) on every element of RDD and return RDD\n",
    "# NOTE:- The dataframe doesn't have map() transformation to use with dataframe you need to generate RDD first !\n",
    "\n",
    "data = [('vinay','kumar'),('sai','raj'),('ajay','row')]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "print(rdd.collect())\n",
    "rdd1 = rdd.map(lambda x : (x[0]+x[1],))# now it is tuple because of (,) at the end with out that it would have become a list !\n",
    "print(rdd1.collect())\n",
    "\n",
    "# This is how we can use the Map() function on the rdd data !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "942af766-bf0d-4add-a930-8391ea5d64d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##üç¨Now we will see how to use the `flatMap()` function in pyspark !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d8a8b2f-b77a-491c-a151-b0d285af5968",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vinay kumar\nsai raj\najay row\n[['vinay', 'kumar'], ['sai', 'raj'], ['ajay', 'row']]\n['vinay', 'kumar', 'sai', 'raj', 'ajay', 'row']\n"
     ]
    }
   ],
   "source": [
    "#The flatMap() function is a transformation operation that flattens the RDD ! After applying the function on every element and returns a mew pyspark RDD!\n",
    "#It act's like the .explode() function on dataframe ! but works on RDD!\n",
    "\n",
    "data = [('vinay kumar'),('sai raj'),('ajay row')]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "for i in rdd.collect():\n",
    "    print(i)\n",
    "\n",
    "rdd_1 = rdd.map(lambda x:x.split(\" \"))#It will returrn the list !\n",
    "rdd_2 = rdd.flatMap(lambda x:x.split(' '))# This will remove the nested list's inside the list !\n",
    "\n",
    "print(rdd_1.collect())\n",
    "print(rdd_2.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe6a1903-0cf0-4090-8943-920df9643725",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##üé≥Now we will use the `partitionBy()` function in pyspark ! (_IMP_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ddf42d4-cbb4-4d47-b086-a664c4c5ad21",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## üçâIMP:-\n",
    "    The partition is the concept of making the large files in to small files for speed action and read/write operations operation's \n",
    "    the partition is must more important for large data files !\n",
    "\n",
    "    You can make a partition of a dataframe based on the column in that dataframe !\n",
    "    then you will find the differend data files names after the data in the column that you have used in the partition !\n",
    "    and when you view the data in that file then you won't find the column name that you have used in those files because \n",
    "    based on that column they were differed so there is no reason to keep those columns in that data files !\n",
    "\n",
    "    ! You can make miltiple data partitions on the came column on differend locations and you can even make an list of columns\n",
    "    that you need to partition !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c8a1fab-ae28-4706-9803-9d6f9b21e81e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+------+------+\n| Id|  name|salary|exp|gender|domain|\n+---+------+------+---+------+------+\n|  1|maheer|  2008|  2|     M| Cloud|\n|  2|  wafa|  3000|  1|     F| Cloud|\n|  3|  abcd|  1000|  4|     F|    HR|\n+---+------+------+---+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# partitioning is a way to split the data into multiple partitions so that you can execute transformations \n",
    "# on multiple partitions in parallel which allows completing the job faster.\n",
    "# You can also write partitioned data into a file system (multiple sub-directories) for faster reads by downstream systems.\n",
    "# In order to perform this partitioning in the pyspark we use the partitionBy() function !\n",
    "\n",
    "datal = [(1, 'maheer' ,2008,2,'M','Cloud'),(2, 'wafa' ,3000,1,'F','Cloud'),(3, 'abcd' ,1000,4,'F','HR')]\n",
    "schemal = ['Id', 'name', 'salary','exp','gender','domain']\n",
    "df = spark.createDataFrame(datal,schemal)\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91a2354d-1004-4ebe-9e7c-3c73b3c26833",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Now we will write this to the local DBFS in databricks ! and apply the partiction on that !\n",
    "df.write.parquet('/FileStore/partiction_example',mode = 'overwrite',partitionBy = 'domain')# Right now i am trying to partiction the domain column in the given\n",
    "# File path !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19344714-47f4-4086-8805-79e616d6aca2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+------+------+\n| Id|  name|salary|exp|gender|domain|\n+---+------+------+---+------+------+\n|  1|maheer|  2008|  2|     M| Cloud|\n|  2|  wafa|  3000|  1|     F| Cloud|\n|  3|  abcd|  1000|  4|     F|    HR|\n+---+------+------+---+------+------+\n\n+---+------+------+---+------+\n| Id|  name|salary|exp|gender|\n+---+------+------+---+------+\n|  1|maheer|  2008|  2|     M|\n|  2|  wafa|  3000|  1|     F|\n+---+------+------+---+------+\n\n+---+----+------+---+------+\n| Id|name|salary|exp|gender|\n+---+----+------+---+------+\n|  3|abcd|  1000|  4|     F|\n+---+----+------+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "# We will see a separate files in the given file path and those files are 1)--> Cloud and 2)--> HR \n",
    "\n",
    "# Now we will access those partiction file data !\n",
    "spark.read.parquet('/FileStore/partiction_example').show()\n",
    "\n",
    "# Now we will only like to see the partiction data of domail Cloud !\n",
    "spark.read.parquet('dbfs:/FileStore/partiction_example/domain=Cloud').show()\n",
    "# But we cannot find the domain column in the partioned data file because the data is already filtered based on the domail as Cloud so it is no use \n",
    "# To save that # column in the dataframe\n",
    "\n",
    "# Now we will only like to see the partiction data of domail HR !\n",
    "spark.read.parquet('dbfs:/FileStore/partiction_example/domain=HR').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1980b023-2133-4f7d-8cf5-e218e2fb0774",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# we can do the partition with more then one column like you can take the column name and againg partition the other column inside that column !\n",
    "\n",
    "df.write.parquet('/FileStore/partiction_example/miltiple_partition',mode =\"overwrite\",partitionBy = ['gender','domain'])\n",
    "\n",
    "# In this above line i am partitining the dataframe based on gender and inside i am again partition the data by domain !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0224bbf8-f12b-4b86-966a-15671d581bd9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+------+------+\n| Id|  name|salary|exp|gender|domain|\n+---+------+------+---+------+------+\n|  1|maheer|  2008|  2|     M| Cloud|\n|  2|  wafa|  3000|  1|     F| Cloud|\n|  3|  abcd|  1000|  4|     F|    HR|\n+---+------+------+---+------+------+\n\n+---+------+------+---+------+\n| Id|  name|salary|exp|domain|\n+---+------+------+---+------+\n|  1|maheer|  2008|  2| Cloud|\n+---+------+------+---+------+\n\n+---+----+------+---+------+\n| Id|name|salary|exp|domain|\n+---+----+------+---+------+\n|  2|wafa|  3000|  1| Cloud|\n|  3|abcd|  1000|  4|    HR|\n+---+----+------+---+------+\n\n+---+----+------+---+\n| Id|name|salary|exp|\n+---+----+------+---+\n|  2|wafa|  3000|  1|\n+---+----+------+---+\n\n"
     ]
    }
   ],
   "source": [
    "# Now we will access the saved data !\n",
    "spark.read.parquet('/FileStore/partiction_example/miltiple_partition').show()# It will show all the data ! combined \n",
    "\n",
    "\n",
    "# Now i need only the gender data of male !\n",
    "spark.read.parquet('dbfs:/FileStore/partiction_example/miltiple_partition/gender=M').show()# This will show the male data !\n",
    "\n",
    "# Now i need only the gender data of female !\n",
    "spark.read.parquet('dbfs:/FileStore/partiction_example/miltiple_partition/gender=F').show()# This will show the female data !\n",
    "\n",
    "# Now i need the data of a gender femail and domain is cloud !\n",
    "spark.read.parquet('dbfs:/FileStore/partiction_example/miltiple_partition/gender=F/domain=Cloud').show()\n",
    "\n",
    "# _--------------------------------------------------This is how the partitionBy works in pyspark IMP-------------------------------------------_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a03f06e-d3e3-471a-aa06-7ad4e3d8e40d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##üéíNow we will see how to use the `from_json()` function in pyspark !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46647dc9-446c-4e63-814f-a5313b3bd47c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Name: string (nullable = true)\n |-- Exp: string (nullable = true)\n\n+-----+----------------------------------------------+\n|Name |Exp                                           |\n+-----+----------------------------------------------+\n|Vinay|{\"domains\":[\"Hr\",\"Cloud\"]}                    |\n|Kumar|{\"domains\":[\"Data_Engineer\",\"Data_Scientist\"]}|\n+-----+----------------------------------------------+\n\n+-----+----------------------------------------------+--------------------------------------------+\n|Name |Exp                                           |New_Exp                                     |\n+-----+----------------------------------------------+--------------------------------------------+\n|Vinay|{\"domains\":[\"Hr\",\"Cloud\"]}                    |{domains -> [Hr, Cloud]}                    |\n|Kumar|{\"domains\":[\"Data_Engineer\",\"Data_Scientist\"]}|{domains -> [Data_Engineer, Data_Scientist]}|\n+-----+----------------------------------------------+--------------------------------------------+\n\n+--------------------+-------------------+\n|             New_Exp|New_Exp[domains][0]|\n+--------------------+-------------------+\n|{domains -> [Hr, ...|                 Hr|\n|{domains -> [Data...|      Data_Engineer|\n+--------------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# The from_json() function is used to convert json string into MapType ! The MapType is like a dict !\n",
    "\n",
    "data = [('Vinay', '{\"domains\":[\"Hr\",\"Cloud\"]}'), ('Kumar', '{\"domains\":[\"Data_Engineer\",\"Data_Scientist\"]}')]\n",
    "sch = ['Name','Exp']\n",
    "df = spark.createDataFrame(data,sch)\n",
    "df.printSchema()\n",
    "df.show(truncate = False)\n",
    "\n",
    "# Now the data is in json string formate now i am going to convert that json string to mapType()\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "sch = MapType(StringType(),ArrayType(StringType()))\n",
    "df = df.withColumn('New_Exp',from_json(df.Exp,sch))\n",
    "df.show(truncate = False)\n",
    "\n",
    "# This is how we can convert the json stringType() to the MapType()\n",
    "df.select('New_Exp',df.New_Exp.domains[0]).show() #1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce9e1ee8-b65e-49cc-ae94-8ec635d19175",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# üçÉNow i am trying to connect with the mongoDB !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31686e2e-5bc2-4a53-aeba-0af66faec41f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mServerSelectionTimeoutError\u001B[0m               Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3046991485909795>:33\u001B[0m\n",
       "\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m record \u001B[38;5;129;01min\u001B[39;00m data:\n",
       "\u001B[1;32m     28\u001B[0m     document \u001B[38;5;241m=\u001B[39m {\n",
       "\u001B[1;32m     29\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m'\u001B[39m: record[\u001B[38;5;241m0\u001B[39m],\n",
       "\u001B[1;32m     30\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m: record[\u001B[38;5;241m1\u001B[39m],\n",
       "\u001B[1;32m     31\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mroles\u001B[39m\u001B[38;5;124m'\u001B[39m: record[\u001B[38;5;241m2\u001B[39m]\n",
       "\u001B[1;32m     32\u001B[0m     }\n",
       "\u001B[0;32m---> 33\u001B[0m     collection\u001B[38;5;241m.\u001B[39minsert_one(document)\n",
       "\u001B[1;32m     35\u001B[0m \u001B[38;5;66;03m# Close the MongoDB connection\u001B[39;00m\n",
       "\u001B[1;32m     36\u001B[0m client\u001B[38;5;241m.\u001B[39mclose()\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/pymongo/collection.py:669\u001B[0m, in \u001B[0;36mCollection.insert_one\u001B[0;34m(self, document, bypass_document_validation, session, comment)\u001B[0m\n",
       "\u001B[1;32m    665\u001B[0m     document[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_id\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m ObjectId()  \u001B[38;5;66;03m# type: ignore[index]\u001B[39;00m\n",
       "\u001B[1;32m    667\u001B[0m write_concern \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write_concern_for(session)\n",
       "\u001B[1;32m    668\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m InsertOneResult(\n",
       "\u001B[0;32m--> 669\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_insert_one\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m    670\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdocument\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    671\u001B[0m \u001B[43m        \u001B[49m\u001B[43mordered\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    672\u001B[0m \u001B[43m        \u001B[49m\u001B[43mwrite_concern\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwrite_concern\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    673\u001B[0m \u001B[43m        \u001B[49m\u001B[43mop_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    674\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbypass_doc_val\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbypass_document_validation\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    675\u001B[0m \u001B[43m        \u001B[49m\u001B[43msession\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msession\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    676\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcomment\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcomment\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    677\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m,\n",
       "\u001B[1;32m    678\u001B[0m     write_concern\u001B[38;5;241m.\u001B[39macknowledged,\n",
       "\u001B[1;32m    679\u001B[0m )\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/pymongo/collection.py:609\u001B[0m, in \u001B[0;36mCollection._insert_one\u001B[0;34m(self, doc, ordered, write_concern, op_id, bypass_doc_val, session, comment)\u001B[0m\n",
       "\u001B[1;32m    597\u001B[0m     result \u001B[38;5;241m=\u001B[39m conn\u001B[38;5;241m.\u001B[39mcommand(\n",
       "\u001B[1;32m    598\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__database\u001B[38;5;241m.\u001B[39mname,\n",
       "\u001B[1;32m    599\u001B[0m         command,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    604\u001B[0m         retryable_write\u001B[38;5;241m=\u001B[39mretryable_write,\n",
       "\u001B[1;32m    605\u001B[0m     )\n",
       "\u001B[1;32m    607\u001B[0m     _check_write_command_response(result)\n",
       "\u001B[0;32m--> 609\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__database\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_retryable_write\u001B[49m\u001B[43m(\u001B[49m\u001B[43macknowledged\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_insert_command\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msession\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    611\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(doc, RawBSONDocument):\n",
       "\u001B[1;32m    612\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m doc\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_id\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/pymongo/mongo_client.py:1522\u001B[0m, in \u001B[0;36mMongoClient._retryable_write\u001B[0;34m(self, retryable, func, session, bulk)\u001B[0m\n",
       "\u001B[1;32m   1502\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_retryable_write\u001B[39m(\n",
       "\u001B[1;32m   1503\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n",
       "\u001B[1;32m   1504\u001B[0m     retryable: \u001B[38;5;28mbool\u001B[39m,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   1507\u001B[0m     bulk: Optional[_Bulk] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n",
       "\u001B[1;32m   1508\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m T:\n",
       "\u001B[1;32m   1509\u001B[0m     \u001B[38;5;124;03m\"\"\"Execute an operation with consecutive retries if possible\u001B[39;00m\n",
       "\u001B[1;32m   1510\u001B[0m \n",
       "\u001B[1;32m   1511\u001B[0m \u001B[38;5;124;03m    Returns func()'s return value on success. On error retries the same\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   1520\u001B[0m \u001B[38;5;124;03m      - `bulk`: bulk abstraction to execute operations in bulk, defaults to None\u001B[39;00m\n",
       "\u001B[1;32m   1521\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 1522\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tmp_session(session) \u001B[38;5;28;01mas\u001B[39;00m s:\n",
       "\u001B[1;32m   1523\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retry_with_session(retryable, func, s, bulk)\n",
       "\n",
       "File \u001B[0;32m/usr/lib/python3.9/contextlib.py:117\u001B[0m, in \u001B[0;36m_GeneratorContextManager.__enter__\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m    115\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkwds, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunc\n",
       "\u001B[1;32m    116\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 117\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgen\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    118\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n",
       "\u001B[1;32m    119\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgenerator didn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt yield\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/pymongo/mongo_client.py:1840\u001B[0m, in \u001B[0;36mMongoClient._tmp_session\u001B[0;34m(self, session, close)\u001B[0m\n",
       "\u001B[1;32m   1837\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m session\n",
       "\u001B[1;32m   1838\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n",
       "\u001B[0;32m-> 1840\u001B[0m s \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_ensure_session\u001B[49m\u001B[43m(\u001B[49m\u001B[43msession\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1841\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m s:\n",
       "\u001B[1;32m   1842\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/pymongo/mongo_client.py:1823\u001B[0m, in \u001B[0;36mMongoClient._ensure_session\u001B[0;34m(self, session)\u001B[0m\n",
       "\u001B[1;32m   1818\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m session\n",
       "\u001B[1;32m   1820\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m   1821\u001B[0m     \u001B[38;5;66;03m# Don't make implicit sessions causally consistent. Applications\u001B[39;00m\n",
       "\u001B[1;32m   1822\u001B[0m     \u001B[38;5;66;03m# should always opt-in.\u001B[39;00m\n",
       "\u001B[0;32m-> 1823\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__start_session\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcausal_consistency\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1824\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (ConfigurationError, InvalidOperation):\n",
       "\u001B[1;32m   1825\u001B[0m     \u001B[38;5;66;03m# Sessions not supported.\u001B[39;00m\n",
       "\u001B[1;32m   1826\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/pymongo/mongo_client.py:1766\u001B[0m, in \u001B[0;36mMongoClient.__start_session\u001B[0;34m(self, implicit, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1763\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__start_session\u001B[39m(\u001B[38;5;28mself\u001B[39m, implicit: \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ClientSession:\n",
       "\u001B[1;32m   1764\u001B[0m     \u001B[38;5;66;03m# Raises ConfigurationError if sessions are not supported.\u001B[39;00m\n",
       "\u001B[1;32m   1765\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m implicit:\n",
       "\u001B[0;32m-> 1766\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_topology\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_implicit_session_support\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1767\u001B[0m         server_session: Union[_EmptyServerSession, _ServerSession] \u001B[38;5;241m=\u001B[39m _EmptyServerSession()\n",
       "\u001B[1;32m   1768\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/pymongo/topology.py:573\u001B[0m, in \u001B[0;36mTopology._check_implicit_session_support\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m    571\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_check_implicit_session_support\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m    572\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n",
       "\u001B[0;32m--> 573\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_session_support\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/pymongo/topology.py:589\u001B[0m, in \u001B[0;36mTopology._check_session_support\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m    585\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_servers_loop(\n",
       "\u001B[1;32m    586\u001B[0m             any_server_selector, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_server_selection_timeout(), \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    587\u001B[0m         )\n",
       "\u001B[1;32m    588\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_description\u001B[38;5;241m.\u001B[39mreadable_servers:\n",
       "\u001B[0;32m--> 589\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_select_servers_loop\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m    590\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreadable_server_selector\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_server_selection_timeout\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\n",
       "\u001B[1;32m    591\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    593\u001B[0m session_timeout \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_description\u001B[38;5;241m.\u001B[39mlogical_session_timeout_minutes\n",
       "\u001B[1;32m    594\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m session_timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/pymongo/topology.py:259\u001B[0m, in \u001B[0;36mTopology._select_servers_loop\u001B[0;34m(self, selector, timeout, address)\u001B[0m\n",
       "\u001B[1;32m    256\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m server_descriptions:\n",
       "\u001B[1;32m    257\u001B[0m     \u001B[38;5;66;03m# No suitable servers.\u001B[39;00m\n",
       "\u001B[1;32m    258\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m now \u001B[38;5;241m>\u001B[39m end_time:\n",
       "\u001B[0;32m--> 259\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m ServerSelectionTimeoutError(\n",
       "\u001B[1;32m    260\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_error_message(selector)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Timeout: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtimeout\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124ms, Topology Description: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdescription\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    261\u001B[0m         )\n",
       "\u001B[1;32m    263\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ensure_opened()\n",
       "\u001B[1;32m    264\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request_check_all()\n",
       "\n",
       "\u001B[0;31mServerSelectionTimeoutError\u001B[0m: SSL handshake failed: ac-iyanxel-shard-00-02.dnbkemf.mongodb.net:27017: [SSL: TLSV1_ALERT_INTERNAL_ERROR] tlsv1 alert internal error (_ssl.c:1129) (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms),SSL handshake failed: ac-iyanxel-shard-00-00.dnbkemf.mongodb.net:27017: [SSL: TLSV1_ALERT_INTERNAL_ERROR] tlsv1 alert internal error (_ssl.c:1129) (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms),SSL handshake failed: ac-iyanxel-shard-00-01.dnbkemf.mongodb.net:27017: [SSL: TLSV1_ALERT_INTERNAL_ERROR] tlsv1 alert internal error (_ssl.c:1129) (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms), Timeout: 30s, Topology Description: <TopologyDescription id: 65a94603f28fbc78a1202abe, topology_type: ReplicaSetNoPrimary, servers: [<ServerDescription ('ac-iyanxel-shard-00-00.dnbkemf.mongodb.net', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('SSL handshake failed: ac-iyanxel-shard-00-00.dnbkemf.mongodb.net:27017: [SSL: TLSV1_ALERT_INTERNAL_ERROR] tlsv1 alert internal error (_ssl.c:1129) (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')>, <ServerDescription ('ac-iyanxel-shard-00-01.dnbkemf.mongodb.net', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('SSL handshake failed: ac-iyanxel-shard-00-01.dnbkemf.mongodb.net:27017: [SSL: TLSV1_ALERT_INTERNAL_ERROR] tlsv1 alert internal error (_ssl.c:1129) (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')>, <ServerDescription ('ac-iyanxel-shard-00-02.dnbkemf.mongodb.net', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('SSL handshake failed: ac-iyanxel-shard-00-02.dnbkemf.mongodb.net:27017: [SSL: TLSV1_ALERT_INTERNAL_ERROR] tlsv1 alert internal error (_ssl.c:1129) (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')>]>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mServerSelectionTimeoutError\u001B[0m               Traceback (most recent call last)\nFile \u001B[0;32m<command-3046991485909795>:33\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m record \u001B[38;5;129;01min\u001B[39;00m data:\n\u001B[1;32m     28\u001B[0m     document \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m     29\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m'\u001B[39m: record[\u001B[38;5;241m0\u001B[39m],\n\u001B[1;32m     30\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m: record[\u001B[38;5;241m1\u001B[39m],\n\u001B[1;32m     31\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mroles\u001B[39m\u001B[38;5;124m'\u001B[39m: record[\u001B[38;5;241m2\u001B[39m]\n\u001B[1;32m     32\u001B[0m     }\n\u001B[0;32m---> 33\u001B[0m     collection\u001B[38;5;241m.\u001B[39minsert_one(document)\n\u001B[1;32m     35\u001B[0m \u001B[38;5;66;03m# Close the MongoDB connection\u001B[39;00m\n\u001B[1;32m     36\u001B[0m client\u001B[38;5;241m.\u001B[39mclose()\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/pymongo/collection.py:669\u001B[0m, in \u001B[0;36mCollection.insert_one\u001B[0;34m(self, document, bypass_document_validation, session, comment)\u001B[0m\n\u001B[1;32m    665\u001B[0m     document[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_id\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m ObjectId()  \u001B[38;5;66;03m# type: ignore[index]\u001B[39;00m\n\u001B[1;32m    667\u001B[0m write_concern \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write_concern_for(session)\n\u001B[1;32m    668\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m InsertOneResult(\n\u001B[0;32m--> 669\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_insert_one\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    670\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdocument\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    671\u001B[0m \u001B[43m        \u001B[49m\u001B[43mordered\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    672\u001B[0m \u001B[43m        \u001B[49m\u001B[43mwrite_concern\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwrite_concern\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    673\u001B[0m \u001B[43m        \u001B[49m\u001B[43mop_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    674\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbypass_doc_val\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbypass_document_validation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    675\u001B[0m \u001B[43m        \u001B[49m\u001B[43msession\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msession\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    676\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcomment\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcomment\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    677\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m    678\u001B[0m     write_concern\u001B[38;5;241m.\u001B[39macknowledged,\n\u001B[1;32m    679\u001B[0m )\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/pymongo/collection.py:609\u001B[0m, in \u001B[0;36mCollection._insert_one\u001B[0;34m(self, doc, ordered, write_concern, op_id, bypass_doc_val, session, comment)\u001B[0m\n\u001B[1;32m    597\u001B[0m     result \u001B[38;5;241m=\u001B[39m conn\u001B[38;5;241m.\u001B[39mcommand(\n\u001B[1;32m    598\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__database\u001B[38;5;241m.\u001B[39mname,\n\u001B[1;32m    599\u001B[0m         command,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    604\u001B[0m         retryable_write\u001B[38;5;241m=\u001B[39mretryable_write,\n\u001B[1;32m    605\u001B[0m     )\n\u001B[1;32m    607\u001B[0m     _check_write_command_response(result)\n\u001B[0;32m--> 609\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__database\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_retryable_write\u001B[49m\u001B[43m(\u001B[49m\u001B[43macknowledged\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_insert_command\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msession\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    611\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(doc, RawBSONDocument):\n\u001B[1;32m    612\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m doc\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_id\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/pymongo/mongo_client.py:1522\u001B[0m, in \u001B[0;36mMongoClient._retryable_write\u001B[0;34m(self, retryable, func, session, bulk)\u001B[0m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_retryable_write\u001B[39m(\n\u001B[1;32m   1503\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   1504\u001B[0m     retryable: \u001B[38;5;28mbool\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1507\u001B[0m     bulk: Optional[_Bulk] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1508\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m T:\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;124;03m\"\"\"Execute an operation with consecutive retries if possible\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \n\u001B[1;32m   1511\u001B[0m \u001B[38;5;124;03m    Returns func()'s return value on success. On error retries the same\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1520\u001B[0m \u001B[38;5;124;03m      - `bulk`: bulk abstraction to execute operations in bulk, defaults to None\u001B[39;00m\n\u001B[1;32m   1521\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1522\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tmp_session(session) \u001B[38;5;28;01mas\u001B[39;00m s:\n\u001B[1;32m   1523\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retry_with_session(retryable, func, s, bulk)\n\nFile \u001B[0;32m/usr/lib/python3.9/contextlib.py:117\u001B[0m, in \u001B[0;36m_GeneratorContextManager.__enter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    115\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkwds, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunc\n\u001B[1;32m    116\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 117\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgen\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[1;32m    119\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgenerator didn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt yield\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/pymongo/mongo_client.py:1840\u001B[0m, in \u001B[0;36mMongoClient._tmp_session\u001B[0;34m(self, session, close)\u001B[0m\n\u001B[1;32m   1837\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m session\n\u001B[1;32m   1838\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m-> 1840\u001B[0m s \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_ensure_session\u001B[49m\u001B[43m(\u001B[49m\u001B[43msession\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1841\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m s:\n\u001B[1;32m   1842\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/pymongo/mongo_client.py:1823\u001B[0m, in \u001B[0;36mMongoClient._ensure_session\u001B[0;34m(self, session)\u001B[0m\n\u001B[1;32m   1818\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m session\n\u001B[1;32m   1820\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1821\u001B[0m     \u001B[38;5;66;03m# Don't make implicit sessions causally consistent. Applications\u001B[39;00m\n\u001B[1;32m   1822\u001B[0m     \u001B[38;5;66;03m# should always opt-in.\u001B[39;00m\n\u001B[0;32m-> 1823\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__start_session\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcausal_consistency\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m   1824\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (ConfigurationError, InvalidOperation):\n\u001B[1;32m   1825\u001B[0m     \u001B[38;5;66;03m# Sessions not supported.\u001B[39;00m\n\u001B[1;32m   1826\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/pymongo/mongo_client.py:1766\u001B[0m, in \u001B[0;36mMongoClient.__start_session\u001B[0;34m(self, implicit, **kwargs)\u001B[0m\n\u001B[1;32m   1763\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__start_session\u001B[39m(\u001B[38;5;28mself\u001B[39m, implicit: \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ClientSession:\n\u001B[1;32m   1764\u001B[0m     \u001B[38;5;66;03m# Raises ConfigurationError if sessions are not supported.\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m implicit:\n\u001B[0;32m-> 1766\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_topology\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_implicit_session_support\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1767\u001B[0m         server_session: Union[_EmptyServerSession, _ServerSession] \u001B[38;5;241m=\u001B[39m _EmptyServerSession()\n\u001B[1;32m   1768\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/pymongo/topology.py:573\u001B[0m, in \u001B[0;36mTopology._check_implicit_session_support\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    571\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_check_implicit_session_support\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    572\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m--> 573\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_session_support\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/pymongo/topology.py:589\u001B[0m, in \u001B[0;36mTopology._check_session_support\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    585\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_servers_loop(\n\u001B[1;32m    586\u001B[0m             any_server_selector, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_server_selection_timeout(), \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    587\u001B[0m         )\n\u001B[1;32m    588\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_description\u001B[38;5;241m.\u001B[39mreadable_servers:\n\u001B[0;32m--> 589\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_select_servers_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    590\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreadable_server_selector\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_server_selection_timeout\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\n\u001B[1;32m    591\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    593\u001B[0m session_timeout \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_description\u001B[38;5;241m.\u001B[39mlogical_session_timeout_minutes\n\u001B[1;32m    594\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m session_timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/pymongo/topology.py:259\u001B[0m, in \u001B[0;36mTopology._select_servers_loop\u001B[0;34m(self, selector, timeout, address)\u001B[0m\n\u001B[1;32m    256\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m server_descriptions:\n\u001B[1;32m    257\u001B[0m     \u001B[38;5;66;03m# No suitable servers.\u001B[39;00m\n\u001B[1;32m    258\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m now \u001B[38;5;241m>\u001B[39m end_time:\n\u001B[0;32m--> 259\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m ServerSelectionTimeoutError(\n\u001B[1;32m    260\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_error_message(selector)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Timeout: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtimeout\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124ms, Topology Description: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdescription\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    261\u001B[0m         )\n\u001B[1;32m    263\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ensure_opened()\n\u001B[1;32m    264\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request_check_all()\n\n\u001B[0;31mServerSelectionTimeoutError\u001B[0m: SSL handshake failed: ac-iyanxel-shard-00-02.dnbkemf.mongodb.net:27017: [SSL: TLSV1_ALERT_INTERNAL_ERROR] tlsv1 alert internal error (_ssl.c:1129) (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms),SSL handshake failed: ac-iyanxel-shard-00-00.dnbkemf.mongodb.net:27017: [SSL: TLSV1_ALERT_INTERNAL_ERROR] tlsv1 alert internal error (_ssl.c:1129) (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms),SSL handshake failed: ac-iyanxel-shard-00-01.dnbkemf.mongodb.net:27017: [SSL: TLSV1_ALERT_INTERNAL_ERROR] tlsv1 alert internal error (_ssl.c:1129) (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms), Timeout: 30s, Topology Description: <TopologyDescription id: 65a94603f28fbc78a1202abe, topology_type: ReplicaSetNoPrimary, servers: [<ServerDescription ('ac-iyanxel-shard-00-00.dnbkemf.mongodb.net', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('SSL handshake failed: ac-iyanxel-shard-00-00.dnbkemf.mongodb.net:27017: [SSL: TLSV1_ALERT_INTERNAL_ERROR] tlsv1 alert internal error (_ssl.c:1129) (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')>, <ServerDescription ('ac-iyanxel-shard-00-01.dnbkemf.mongodb.net', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('SSL handshake failed: ac-iyanxel-shard-00-01.dnbkemf.mongodb.net:27017: [SSL: TLSV1_ALERT_INTERNAL_ERROR] tlsv1 alert internal error (_ssl.c:1129) (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')>, <ServerDescription ('ac-iyanxel-shard-00-02.dnbkemf.mongodb.net', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('SSL handshake failed: ac-iyanxel-shard-00-02.dnbkemf.mongodb.net:27017: [SSL: TLSV1_ALERT_INTERNAL_ERROR] tlsv1 alert internal error (_ssl.c:1129) (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')>]>",
       "errorSummary": "<span class='ansi-red-fg'>ServerSelectionTimeoutError</span>: SSL handshake failed: ac-iyanxel-shard-00-02.dnbkemf.mongodb.net:27017: [SSL: TLSV1_ALERT_INTERNAL_ERROR] tlsv1 alert internal error (_ssl.c:1129) (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms),SSL handshake failed: ac-iyanxel-shard-00-00.dnbkemf.mongodb.net:27017: [SSL: TLSV1_ALERT_INTERNAL_ERROR] tlsv1 alert internal error (_ssl.c:1129) (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms),SSL handshake failed: ac-iyanxel-shard-00-01.dnbkemf.mongodb.net:27017: [SSL: TLSV1_ALERT_INTERNAL_ERROR] tlsv1 alert internal error (_ssl.c:1129) (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms), Timeout: 30s, Topology Description: <TopologyDescription id: 65a94603f28fbc78a1202abe, topology_type: ReplicaSetNoPrimary, servers: [<ServerDescription ('ac-iyanxel-shard-00-00.dnbkemf.mongodb.net', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('SSL handshake failed: ac-iyanxel-shard-00-00.dnbkemf.mongodb.net:27017: [SSL: TLSV1_ALERT_INTERNAL_ERROR] tlsv1 alert internal error (_ssl.c:1129) (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')>, <ServerDescription ('ac-iyanxel-shard-00-01.dnbkemf.mongodb.net', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('SSL handshake failed: ac-iyanxel-shard-00-01.dnbkemf.mongodb.net:27017: [SSL: TLSV1_ALERT_INTERNAL_ERROR] tlsv1 alert internal error (_ssl.c:1129) (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')>, <ServerDescription ('ac-iyanxel-shard-00-02.dnbkemf.mongodb.net', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('SSL handshake failed: ac-iyanxel-shard-00-02.dnbkemf.mongodb.net:27017: [SSL: TLSV1_ALERT_INTERNAL_ERROR] tlsv1 alert internal error (_ssl.c:1129) (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')>]>",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pymongo\n",
    "\n",
    "# Your MongoDB connection details\n",
    "mongo_uri =\"mongodb+srv://Vinay:VinayKumarKarivena77@cluster0.dnbkemf.mongodb.net/?retryWrites=true&w=majority\" # Update with your MongoDB URI\n",
    "database_name = \"first\"       # Update with your database name\n",
    "collection_name = \"data\"   # Update with your collection name\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (1, 'vinay', ['Hr', 'DeOps', 'Frontend']),\n",
    "    (2, 'kumar', ['marketing', 'DeOps', 'sales']),\n",
    "    (3, 'vinay', ['Tester', 'fullstack', 'Backend']),\n",
    "    (4, 'vijaya', ['Hr', 'cybersecurity', 'tester']),\n",
    "    (3, 'kavya', ['marketing', 'cloud', 'advertise'])\n",
    "]\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = pymongo.MongoClient(mongo_uri)\n",
    "\n",
    "# Select or create the database\n",
    "db = client[database_name]\n",
    "\n",
    "# Select or create the collection\n",
    "collection = db[collection_name]\n",
    "\n",
    "# Insert data into the collection\n",
    "for record in data:\n",
    "    document = {\n",
    "        'id': record[0],\n",
    "        'name': record[1],\n",
    "        'roles': record[2]\n",
    "    }\n",
    "    collection.insert_one(document)\n",
    "\n",
    "# Close the MongoDB connection\n",
    "client.close()\n",
    "print('It is done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "645ed565-b756-4cdb-aaa6-9c28c8d48125",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##ü§ïNow we will see how we can convert the `json_string` into the `StructType()` in pyspark !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49904901-bbb8-49c7-a6a4-560e63247800",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Name: string (nullable = true)\n |-- History_Emp: string (nullable = true)\n\n+-----+------------------------------------+\n|Name |History_Emp                         |\n+-----+------------------------------------+\n|Vinay|{\"domains\":\"HR\",\"Exp_Years\":\"23\"}   |\n|Kumar|{\"domains\":\"Cloud\",\"Exp_Years\":\"12\"}|\n+-----+------------------------------------+\n\nroot\n |-- Name: string (nullable = true)\n |-- History_Emp: string (nullable = true)\n |-- Historyp: struct (nullable = true)\n |    |-- domains: string (nullable = true)\n |    |-- Exp_Years: string (nullable = true)\n\n+-----+------------------------------------+-----------+\n|Name |History_Emp                         |Historyp   |\n+-----+------------------------------------+-----------+\n|Vinay|{\"domains\":\"HR\",\"Exp_Years\":\"23\"}   |{HR, 23}   |\n|Kumar|{\"domains\":\"Cloud\",\"Exp_Years\":\"12\"}|{Cloud, 12}|\n+-----+------------------------------------+-----------+\n\n+-----------+----------------+\n|   Historyp|Historyp.domains|\n+-----------+----------------+\n|   {HR, 23}|              HR|\n|{Cloud, 12}|           Cloud|\n+-----------+----------------+\n\n+-----------+------------------+\n|   Historyp|Historyp.Exp_Years|\n+-----------+------------------+\n|   {HR, 23}|                23|\n|{Cloud, 12}|                12|\n+-----------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [('Vinay', '{\"domains\":\"HR\",\"Exp_Years\":\"23\"}'), ('Kumar', '{\"domains\":\"Cloud\",\"Exp_Years\":\"12\"}')]\n",
    "sch = ['Name','History_Emp']\n",
    "df = spark.createDataFrame(data,sch)\n",
    "df.printSchema()\n",
    "df.show(truncate = False)\n",
    "\n",
    "# Now i will try to convert the json_string type to the StructType() using the from_json() function !\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "sc = StructType([StructField('domains',StringType()),StructField('Exp_Years',StringType())])\n",
    "df_1 = df.withColumn('Historyp',from_json(df.History_Emp,sc))\n",
    "df_1.printSchema()\n",
    "df_1.show(truncate = False)\n",
    "df_1.select('Historyp',col('Historyp').domains).show()\n",
    "df_1.select('Historyp',col('Historyp').Exp_Years).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a31d636-58b6-4f70-95a9-8cf1fcc13760",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##üò≠Now we will see how to use the `to_json()` function in the pyspark !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9255aadd-d8e9-4199-9905-006657b3227a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: string (nullable = true)\n |-- Details: struct (nullable = true)\n |    |-- Dep: string (nullable = true)\n |    |-- Years_exp: string (nullable = true)\n\n+-----+-----------+\n|name |Details    |\n+-----+-----------+\n|Vinay|{HR, 23}   |\n|Kumar|{Cloud, 12}|\n+-----+-----------+\n\n+-----+--------------------------------+\n|name |Details                         |\n+-----+--------------------------------+\n|Vinay|{\"Dep\":\"HR\",\"Years_exp\":\"23\"}   |\n|Kumar|{\"Dep\":\"Cloud\",\"Years_exp\":\"12\"}|\n+-----+--------------------------------+\n\nroot\n |-- name: string (nullable = true)\n |-- Details: string (nullable = true)\n\nroot\n |-- Name: string (nullable = true)\n |-- Data: map (nullable = true)\n |    |-- key: string\n |    |-- value: double (valueContainsNull = true)\n\n+-----+----------------------------+\n|Name |Data                        |\n+-----+----------------------------+\n|vinay|{Height -> 5.8, Age -> 23.0}|\n|kumar|{Height -> 6.2, Age -> 34.0}|\n+-----+----------------------------+\n\nroot\n |-- Name: string (nullable = true)\n |-- Data: string (nullable = true)\n\n+-----+-------------------------+\n|Name |Data                     |\n+-----+-------------------------+\n|vinay|{\"Height\":5.8,\"Age\":23.0}|\n|kumar|{\"Height\":6.2,\"Age\":34.0}|\n+-----+-------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# This function is nothing but in this we will convert the mapType() or StructType() in to the json_string using `to_json()` function !\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "# This is the StructType() data !\n",
    "data = [('Vinay', (\"HR\",\"23\")), ('Kumar', (\"Cloud\",\"12\"))]\n",
    "in_sch = StructType([\n",
    "    StructField('Dep', StringType()),\n",
    "    StructField('Years_exp', StringType())\n",
    "])\n",
    "out_sch = StructType([\n",
    "    StructField('name', StringType()),\n",
    "    StructField('Details', in_sch)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data,out_sch)\n",
    "df.printSchema()\n",
    "df.show(truncate = False)\n",
    "\n",
    "\n",
    "# This is how the Nested StructType() data will look like after it has been converted in the json_string Type it do look like the mapType() like  dictionary!\n",
    "# Now i will convert the StructType() data column into the json_string!\n",
    "df_1 = df.withColumn('Details',to_json(df.Details))\n",
    "df_1.show(truncate = False)\n",
    "df_1.printSchema()\n",
    "\n",
    "# Now i will creata a MapType() data !\n",
    "\n",
    "data_1 = [('vinay',{'Age':23.0,'Height':5.8}),('kumar',{'Age':34.0,'Height':6.2})]\n",
    "schwww = StructType([StructField('Name',StringType()),StructField('Data',MapType(StringType(),DoubleType()))])\n",
    "df_22 = spark.createDataFrame(data_1,schwww)\n",
    "df_22.printSchema()\n",
    "df_22.show(truncate = False)\n",
    "\n",
    "# Now i am going to \n",
    "df_33 = df_22.withColumn('Data',to_json(df_22.Data))\n",
    "df_33.printSchema()\n",
    "df_33.show(truncate = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89282064-a052-43c3-b8fe-b817428b3417",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##ü©≥Now we will see how to use the `json_tuple()` function in the pyspark !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5e42142-31e7-4f2d-a051-24e9d78f5b34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------------------------+\n|Name |Prop                                     |\n+-----+-----------------------------------------+\n|Vinay|{\"age\":\"23\",\"height\":\"5.6\",\"weight\":\"68\"}|\n|Kumar|{\"age\":\"33\",\"height\":\"4.3\",\"weight\":\"44\"}|\n+-----+-----------------------------------------+\n\nroot\n |-- Name: string (nullable = true)\n |-- Prop: string (nullable = true)\n\n+-----+-----------------------------------------+---+------+------+\n|Name |Prop                                     |AGE|HEIGHT|WEIGHT|\n+-----+-----------------------------------------+---+------+------+\n|Vinay|{\"age\":\"23\",\"height\":\"5.6\",\"weight\":\"68\"}|23 |5.6   |68    |\n|Kumar|{\"age\":\"33\",\"height\":\"4.3\",\"weight\":\"44\"}|33 |4.3   |44    |\n+-----+-----------------------------------------+---+------+------+\n\n+-----+-----------------------------------------+---+------+------+\n|Name |Prop                                     |AGE|HEIGHT|WEIGHT|\n+-----+-----------------------------------------+---+------+------+\n|Vinay|{\"age\":\"23\",\"height\":\"5.6\",\"weight\":\"68\"}|23 |5.6   |68    |\n|Kumar|{\"age\":\"33\",\"height\":\"4.3\",\"weight\":\"44\"}|33 |4.3   |44    |\n+-----+-----------------------------------------+---+------+------+\n\nroot\n |-- Name: string (nullable = true)\n |-- Prop: string (nullable = true)\n |-- AGE: integer (nullable = true)\n |-- HEIGHT: float (nullable = true)\n |-- WEIGHT: integer (nullable = true)\n\n+-----+\n|  yep|\n+-----+\n|Vinay|\n| nope|\n+-----+\n\n+-----+\n| Name|\n+-----+\n|Vinay|\n|Kumar|\n+-----+\n\n+-----+---+------+------+\n|Name |AGE|HEIGHT|WEIGHT|\n+-----+---+------+------+\n|Vinay|23 |5.6   |68    |\n|Kumar|33 |4.3   |44    |\n+-----+---+------+------+\n\n+----+----+----+----+\n|name|age |city|zip |\n+----+----+----+----+\n|null|null|null|null|\n|null|null|null|null|\n+----+----+----+----+\n\n"
     ]
    }
   ],
   "source": [
    "# This json_tuple() function is used to extract or query elements from the json string column and create as a new columns !\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "# Now i will create a data \n",
    "data = [('Vinay','{\"age\":\"23\",\"height\":\"5.6\",\"weight\":\"68\"}'),('Kumar','{\"age\":\"33\",\"height\":\"4.3\",\"weight\":\"44\"}')]\n",
    "sch = ['Name','Prop']\n",
    "# In this data i have made the StructType() or a MapType() data in to the json_string type ()\n",
    "df = spark.createDataFrame(data,sch)\n",
    "df.show(truncate=False)\n",
    "df.printSchema()\n",
    "\n",
    "# Now what i need to do is that i want to take each element from the json_string 'Prop' and make a new columns and stores the data in that row !\n",
    "# With out converting the json_string column to the mapType() or a StructType() ! I want it to happen directly !\n",
    "\n",
    "df_1 = df.withColumn('AGE',json_tuple(df.Prop,'age')).withColumn('HEIGHT',json_tuple(df.Prop,'height')).withColumn('WEIGHT',json_tuple(df.Prop,'weight'))\n",
    "\n",
    "df_1.show(truncate = False)\n",
    "\n",
    "\n",
    "# Now as you can see that the data columns that we have added to the dataframe are not having the right datatype's to it \n",
    "# we can change that using with column and .cast() but there is an other way we can do that using the json_tuple() function !\n",
    "\n",
    "# I am defiing the schema that i need the json_tuple() to get !\n",
    "\n",
    "\n",
    "\n",
    "df_1 = df_1.withColumn(\"AGE\",col('AGE').cast('Int')).withColumn(\"HEIGHT\",col('HEIGHT').cast('Float')).withColumn(\"WEIGHT\",col('WEIGHT').cast('Int'))\n",
    "df_1.show(truncate = False)\n",
    "df_1.printSchema()\n",
    "\n",
    "df_1.select(when(df_1.AGE < 25,df_1.Name).otherwise('nope').alias('yep')).show()\n",
    "\n",
    "df_1.filter(df_1.AGE >21).select(df_1.Name).show()\n",
    "\n",
    "df.select('Name',json_tuple('Prop','age','height','weight').alias('AGE','HEIGHT','WEIGHT')).show(truncate=False) #This is also a valid way of \n",
    "# accesing the columns from the json string usin gthe json_tuple() function !\n",
    "\n",
    "\n",
    "# NOTE:- we can only use the json_tuple() function on the flat json string containes only one mapType in the\n",
    "# If u want to perform the ectraction of the columns which are inside the nested maptype() inside the json string then \n",
    "# We have use the get_json_object() function !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "699b5723-12c0-416c-9074-b5777f1fe522",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##ü•∂Now we will see how to use the `get_json_object()` function in pyspark !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "406205d7-8971-486d-9061-2c994c643fda",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------------------------------------------------+\n|Name  |Props                                                        |\n+------+-------------------------------------------------------------+\n|vinay |{'address':{'city':'Kurnool','state':'AP'},'gender':'male'}  |\n|kumari|{'address':{'city':'Nandyal','state':'AP'},'gender':'female'}|\n+------+-------------------------------------------------------------+\n\nroot\n |-- Name: string (nullable = true)\n |-- Props: string (nullable = true)\n\n+------+-------+\n|  Name|   City|\n+------+-------+\n| vinay|Kurnool|\n|kumari|Nandyal|\n+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# This get_json_object() function is used to extrace the json string based on the path from the json column !\n",
    "\n",
    "data = [('vinay',\"{'address':{'city':'Kurnool','state':'AP'},'gender':'male'}\"),\n",
    "        ('kumari',\"{'address':{'city':'Nandyal','state':'AP'},'gender':'female'}\")]\n",
    "sch = ['Name','Props']\n",
    "df = spark.createDataFrame(data,sch)\n",
    "df.show(truncate = False)\n",
    "df.printSchema()\n",
    "df.select('Name',get_json_object('props','$.address.city').alias('City')).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9da0c604-e2c2-4b58-8473-9e1bb6dd7fed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##üëªThe Difference between `json_tuple()` function and the `get_json_object()` function in pyspark !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec198872-29d3-483f-961a-85d47872d7b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####While both json_tuple() and get_json_object() functions in PySpark are used for extracting data from JSON strings, there are key differences in their functionalities and use cases:\n",
    "\n",
    "\n",
    "\n",
    "1) -----> (_Multiple Values vs. Single Value_):\n",
    "\n",
    "    json_tuple() is designed for extracting multiple values from a JSON string simultaneously. It returns a tuple containing the specified values.\n",
    "    get_json_object(), on the other hand, is focused on extracting a single value at a time. It is used when you want to retrieve a specific value based on a JSON path expression.\n",
    "\n",
    "2) -----> (_Usage with Nested Structures_):\n",
    "\n",
    "    json_tuple() is well-suited for handling nested structures in JSON strings. You can use dot notation to specify nested keys and extract values from within those nested structures.\n",
    "    get_json_object() also supports JSON path expressions, allowing you to navigate through nested structures and retrieve values. However, it is generally used for extracting individual values rather than multiple values from nested structures.\n",
    "\n",
    "3) -----> (_Flexibility_):\n",
    "\n",
    "    json_tuple() provides more flexibility when you need to extract several values from different parts of the JSON string, especially if they are nested.\n",
    "    get_json_object() is more focused on simplicity and is often used when you need to extract specific values from a flat or moderately nested JSON structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4334dfab-de48-4125-96f9-8e78f6786e5c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##üòµ‚Äçüí´No we will see how to use the `current_date()` , `to_data()` and `data_format()` functions in the pyspark !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ab2859c-fdd1-4b1e-b8b4-5708250e802a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n| id|\n+---+\n|  1|\n|  2|\n|  3|\n|  4|\n|  5|\n|  6|\n+---+\n\n+---+-----------+\n|id |currentDate|\n+---+-----------+\n|1  |2024-01-20 |\n|2  |2024-01-20 |\n|3  |2024-01-20 |\n|4  |2024-01-20 |\n|5  |2024-01-20 |\n|6  |2024-01-20 |\n+---+-----------+\n\nroot\n |-- id: long (nullable = false)\n |-- currentDate: date (nullable = false)\n\nroot\n |-- id: long (nullable = false)\n |-- currentDate: string (nullable = false)\n\n+---+--------------+\n| id|   currentDate|\n+---+--------------+\n|  1|2024/^\\00/^\\20|\n|  2|2024/^\\00/^\\20|\n|  3|2024/^\\00/^\\20|\n|  4|2024/^\\00/^\\20|\n|  5|2024/^\\00/^\\20|\n|  6|2024/^\\00/^\\20|\n+---+--------------+\n\n+---+-----------+\n| id|currentDate|\n+---+-----------+\n|  1| 2024-01-20|\n|  2| 2024-01-20|\n|  3| 2024-01-20|\n|  4| 2024-01-20|\n|  5| 2024-01-20|\n|  6| 2024-01-20|\n+---+-----------+\n\nroot\n |-- id: long (nullable = false)\n |-- currentDate: date (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(1,7)\n",
    "df.show()\n",
    "\n",
    "# Now i would like to add the new column that hold's the current date !\n",
    "from pyspark.sql.functions import *\n",
    "df = df.withColumn('currentDate',current_date())\n",
    "df.show(truncate = False)\n",
    "df.printSchema()\n",
    "\n",
    "# Now what if i want to change the formate of the data ! in the currentDate column !\n",
    "\n",
    "df = df.withColumn('currentDate',date_format(df.currentDate,'yyyy/^\\mm/^\\dd'))\n",
    "df.printSchema()# After changing the format of the currentDate is changed to the string !\n",
    "df.show()\n",
    "# Now i wan tto change the currentDate into the default format and datatype also then !\n",
    "\n",
    "df = df.withColumn('currentDate',to_date(df.currentDate,'yyyy/^\\mm/^\\dd'))# In this you have to specify the format of the date in that column ! then to_date will \n",
    "# Change that date string to the actual date dataType()\n",
    "df.show()\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4be3cf3-64ad-4bfe-a35d-6f76c7d996ac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##ü§¨Now we will see how to use the `datediff()` , `months_between()` , `add_months()` , `date_add()` , `month()` , `year()` function in the pyspark !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc2cbc53-0691-4f5a-a344-726c649f7529",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n|    date_1|    date_2|\n+----------+----------+\n|2015-06-14|2015-07-14|\n+----------+----------+\n\n+----------+----------+--------+\n|    date_1|    date_2|dateDiff|\n+----------+----------+--------+\n|2015-06-14|2015-07-14|      30|\n+----------+----------+--------+\n\n+----------+----------+--------+----------+\n|    date_1|    date_2|dateDiff|monthsBetw|\n+----------+----------+--------+----------+\n|2015-06-14|2015-07-14|      30|       1.0|\n+----------+----------+--------+----------+\n\n+----------+----------+--------+----------+----------+----------+\n|    date_1|    date_2|dateDiff|monthsBetw| addMonths| subMonths|\n+----------+----------+--------+----------+----------+----------+\n|2015-06-14|2015-07-14|      30|       1.0|2016-01-14|2014-12-14|\n+----------+----------+--------+----------+----------+----------+\n\n+----------+----------+--------+----------+----------+----------+----------+----------+\n|    date_1|    date_2|dateDiff|monthsBetw| addMonths| subMonths|   addDays|   subDays|\n+----------+----------+--------+----------+----------+----------+----------+----------+\n|2015-06-14|2015-07-14|      30|       1.0|2016-01-14|2014-12-14|2015-06-26|2015-06-02|\n+----------+----------+--------+----------+----------+----------+----------+----------+\n\n+----------+----------+--------+----------+----------+----------+----------+----------+------------------+-----------------+\n|    date_1|    date_2|dateDiff|monthsBetw| addMonths| subMonths|   addDays|   subDays|month_in_the_date!|year_in_the_date!|\n+----------+----------+--------+----------+----------+----------+----------+----------+------------------+-----------------+\n|2015-06-14|2015-07-14|      30|       1.0|2016-01-14|2014-12-14|2015-06-26|2015-06-02|                 6|             2015|\n+----------+----------+--------+----------+----------+----------+----------+----------+------------------+-----------------+\n\nroot\n |-- date_1: string (nullable = true)\n |-- date_2: string (nullable = true)\n |-- dateDiff: integer (nullable = true)\n |-- monthsBetw: double (nullable = true)\n |-- addMonths: date (nullable = true)\n |-- subMonths: date (nullable = true)\n |-- addDays: date (nullable = true)\n |-- subDays: date (nullable = true)\n |-- month_in_the_date!: integer (nullable = true)\n |-- year_in_the_date!: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([('2015-06-14','2015-07-14')],['date_1','date_2'])\n",
    "df.show()\n",
    "\n",
    "df = df.withColumn('dateDiff',datediff(df.date_2,df.date_1))# This will show how many day's of difference !\n",
    "df.show()\n",
    "\n",
    "df = df.withColumn('monthsBetw',months_between(df.date_2,df.date_1))# This will show how many month's of difference !\n",
    "df.show()\n",
    "\n",
    "df = df.withColumn('addMonths',add_months(df.date_2,6))# This will add the months to the provided date with the no of months provided !\n",
    "df = df.withColumn('subMonths',add_months(df.date_1,-6))# This will subtract the months to the provided date with the no of months provided in the '-' sigh !\n",
    "df.show()\n",
    "\n",
    "df = df.withColumn('addDays',date_add(df.date_1,12))# This will add the day's to the specified date that is provided !\n",
    "df = df.withColumn('subDays',date_add(df.date_1,-12))# This will subtract the day's to the specified date that is provided  with the '-' sign !\n",
    "df.show()\n",
    "\n",
    "df = df.withColumn('month_in_the_date!',month(df.date_1))\n",
    "df = df.withColumn('year_in_the_date!',year(df.date_1))\n",
    "df.show()\n",
    "df.printSchema()\n",
    "\n",
    "# NOTE:-\n",
    "        # At first we have created the date in the tuple as a string the easy way to convert the string date in to date datatype is by using the \n",
    "        # date_format() and add_months() with '0' months adding !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26241c3c-bdf2-4b2f-b0cb-9cfe473cac00",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##ü§¢Now we will see how to work with `Timestamp` function in pyspark !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc769da0-8505-4849-9e3b-1b81c8709a30",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+\n|id |currentTimestamp       |\n+---+-----------------------+\n|1  |2024-01-20 15:53:55.275|\n|2  |2024-01-20 15:53:55.275|\n|3  |2024-01-20 15:53:55.275|\n|4  |2024-01-20 15:53:55.275|\n|5  |2024-01-20 15:53:55.275|\n|6  |2024-01-20 15:53:55.275|\n|7  |2024-01-20 15:53:55.275|\n+---+-----------------------+\n\n+---+-----------------------+-----------------------+\n|id |currentTimestamp       |timestampInString      |\n+---+-----------------------+-----------------------+\n|1  |2024-01-20 15:53:55.474|23//01//2022 02//23//23|\n|2  |2024-01-20 15:53:55.474|23//01//2022 02//23//23|\n|3  |2024-01-20 15:53:55.474|23//01//2022 02//23//23|\n|4  |2024-01-20 15:53:55.474|23//01//2022 02//23//23|\n|5  |2024-01-20 15:53:55.474|23//01//2022 02//23//23|\n|6  |2024-01-20 15:53:55.474|23//01//2022 02//23//23|\n|7  |2024-01-20 15:53:55.474|23//01//2022 02//23//23|\n+---+-----------------------+-----------------------+\n\nroot\n |-- id: long (nullable = false)\n |-- currentTimestamp: timestamp (nullable = false)\n |-- timestampInString: string (nullable = false)\n\n+---+-----------------------+-----------------------+-------------------+\n|id |currentTimestamp       |timestampInString      |StringToTimestamp  |\n+---+-----------------------+-----------------------+-------------------+\n|1  |2024-01-20 15:53:55.723|23//01//2022 02//23//23|2022-01-01 02:23:23|\n|2  |2024-01-20 15:53:55.723|23//01//2022 02//23//23|2022-01-01 02:23:23|\n|3  |2024-01-20 15:53:55.723|23//01//2022 02//23//23|2022-01-01 02:23:23|\n|4  |2024-01-20 15:53:55.723|23//01//2022 02//23//23|2022-01-01 02:23:23|\n|5  |2024-01-20 15:53:55.723|23//01//2022 02//23//23|2022-01-01 02:23:23|\n|6  |2024-01-20 15:53:55.723|23//01//2022 02//23//23|2022-01-01 02:23:23|\n|7  |2024-01-20 15:53:55.723|23//01//2022 02//23//23|2022-01-01 02:23:23|\n+---+-----------------------+-----------------------+-------------------+\n\nroot\n |-- id: long (nullable = false)\n |-- currentTimestamp: timestamp (nullable = false)\n |-- timestampInString: string (nullable = false)\n |-- StringToTimestamp: timestamp (nullable = true)\n\n+---+-----------------------+-----------------------+-------------------+----+------+------+\n|id |currentTimestamp       |timestampInString      |StringToTimestamp  |Hour|Minute|Second|\n+---+-----------------------+-----------------------+-------------------+----+------+------+\n|1  |2024-01-20 15:53:56.161|23//01//2022 02//23//23|2022-01-01 02:23:23|15  |53    |56    |\n|2  |2024-01-20 15:53:56.161|23//01//2022 02//23//23|2022-01-01 02:23:23|15  |53    |56    |\n|3  |2024-01-20 15:53:56.161|23//01//2022 02//23//23|2022-01-01 02:23:23|15  |53    |56    |\n|4  |2024-01-20 15:53:56.161|23//01//2022 02//23//23|2022-01-01 02:23:23|15  |53    |56    |\n|5  |2024-01-20 15:53:56.161|23//01//2022 02//23//23|2022-01-01 02:23:23|15  |53    |56    |\n|6  |2024-01-20 15:53:56.161|23//01//2022 02//23//23|2022-01-01 02:23:23|15  |53    |56    |\n|7  |2024-01-20 15:53:56.161|23//01//2022 02//23//23|2022-01-01 02:23:23|15  |53    |56    |\n+---+-----------------------+-----------------------+-------------------+----+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# The timestampTypw default fomart is 'yyyy-mm-dd hh:mm:ss.SS'\n",
    "# current_timestamp() get the current timestamp. by default the data will be returned in default fomart !\n",
    "# to_timestamp() converts timestamps in to TimestampType. We need to specify fomart of timestamp in the string in the function !\n",
    "# hour() and minute(),second() function are to be used on the timestamp !!!\n",
    "\n",
    "df = spark.range(1,8)\n",
    "df= df.withColumn('currentTimestamp',current_timestamp()) #This will give the current timestamp !\n",
    "df.show(truncate = False)\n",
    "\n",
    "df = df.withColumn('timestampInString',lit('23//01//2022 02//23//23'))\n",
    "df.show(truncate = False)\n",
    "df.printSchema()\n",
    "\n",
    "# Now i will convert the timestampInString column stringdata in to the timestamp in a new column !\n",
    "\n",
    "df = df.withColumn('StringToTimestamp',to_timestamp(df.timestampInString,'mm//dd//yyyy hh//mm//ss'))# We have to provide the string format !\n",
    "df.show(truncate = False)\n",
    "df.printSchema()\n",
    "\n",
    "df.select('*',hour(df.currentTimestamp).alias('Hour'),minute(df.currentTimestamp).alias('Minute'),second(df.currentTimestamp).alias('Second')).show(truncate = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "690bf9ab-4e12-4c76-bf42-d14cd63f542c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##üíÄNow i would like to use the aggregate functions in pyspark like `approx_count_distinct()` , `avg()` , `collect_list()` , `collect_set()` , `countDistinct()` , `Count()` !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "660b2d1e-a02f-464f-af96-5f5882c6c83c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+\n| Id|  name|salary|dep|\n+---+------+------+---+\n|  1|maheer|  1000|  2|\n|  2|  wafa|  3000|  1|\n|  3|  abcd|  1000|  4|\n+---+------+------+---+\n\nroot\n |-- Id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- salary: long (nullable = true)\n |-- dep: long (nullable = true)\n\n+-----------------------------+\n|approx_count_distinct(salary)|\n+-----------------------------+\n|                            2|\n+-----------------------------+\n\n+---------------------+\n|round(avg(salary), 2)|\n+---------------------+\n|              1666.67|\n+---------------------+\n\n+--------------------+\n|collect_list(salary)|\n+--------------------+\n|  [1000, 3000, 1000]|\n+--------------------+\n\n+-------------------+\n|collect_set(salary)|\n+-------------------+\n|       [3000, 1000]|\n+-------------------+\n\n+----------------------+\n|count(DISTINCT salary)|\n+----------------------+\n|                     2|\n+----------------------+\n\n+-------------+\n|count(salary)|\n+-------------+\n|            3|\n+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "datal = [(1, 'maheer' ,1000,2),(2, 'wafa' ,3000,1),(3, 'abcd' ,1000,4)]\n",
    "schemal = ['Id', 'name', 'salary','dep']\n",
    "\n",
    "df = spark.createDataFrame(datal,schemal)\n",
    "df.show()\n",
    "df.printSchema()\n",
    "\n",
    "df.select(approx_count_distinct(df.salary)).show()\n",
    "df.select(round(avg(df.salary), 2)).show()# This is how we use the round() function !\n",
    "df.select(collect_list(df.salary)).show()\n",
    "df.select(collect_set(df.salary)).show()\n",
    "df.select(countDistinct(df.salary)).show()\n",
    "df.select(count(df.salary)).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67b1211f-75d3-43e4-960a-b1923c1ef969",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##üêºNow we use the `row_number()` , `rank()` , `dense_rank()` function in the pyspark !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c1ac01a-aada-44d7-9a86-9716073490e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+\n|Name  |Dep    |Salary|\n+------+-------+------+\n|vinay |HR     |30000 |\n|kumar |Cloud  |32000 |\n|sai   |HR     |30000 |\n|bala  |Cloud  |44000 |\n|pooja |Backend|33000 |\n|kumari|Manager|120000|\n|jon   |Cloud  |44000 |\n|sam   |HR     |42000 |\n|raj   |Backend|33000 |\n+------+-------+------+\n\nroot\n |-- Name: string (nullable = true)\n |-- Dep: string (nullable = true)\n |-- Salary: long (nullable = true)\n\n+------+-------+------+---+\n|  Name|    Dep|Salary| Id|\n+------+-------+------+---+\n| pooja|Backend| 33000|  1|\n|   raj|Backend| 33000|  2|\n| kumar|  Cloud| 32000|  3|\n|  bala|  Cloud| 44000|  4|\n|   jon|  Cloud| 44000|  5|\n| vinay|     HR| 30000|  6|\n|   sai|     HR| 30000|  7|\n|   sam|     HR| 42000|  8|\n|kumari|Manager|120000|  9|\n+------+-------+------+---+\n\n+------+-------+------+---------------+\n|  Name|    Dep|Salary|Id_Based_On_Dep|\n+------+-------+------+---------------+\n| pooja|Backend| 33000|              1|\n|   raj|Backend| 33000|              2|\n| kumar|  Cloud| 32000|              1|\n|  bala|  Cloud| 44000|              2|\n|   jon|  Cloud| 44000|              3|\n| vinay|     HR| 30000|              1|\n|   sai|     HR| 30000|              2|\n|   sam|     HR| 42000|              3|\n|kumari|Manager|120000|              1|\n+------+-------+------+---------------+\n\n+------+-------+------+---------------+---+----+\n|  Name|    Dep|Salary|Id_Based_On_Dep| Id|rank|\n+------+-------+------+---------------+---+----+\n| pooja|Backend| 33000|              1|  1|   1|\n|   raj|Backend| 33000|              2|  2|   1|\n| kumar|  Cloud| 32000|              1|  3|   3|\n|  bala|  Cloud| 44000|              2|  4|   3|\n|   jon|  Cloud| 44000|              3|  5|   3|\n| vinay|     HR| 30000|              1|  6|   6|\n|   sai|     HR| 30000|              2|  7|   6|\n|   sam|     HR| 42000|              3|  8|   6|\n|kumari|Manager|120000|              1|  9|   9|\n+------+-------+------+---------------+---+----+\n\n+------+-------+------+---------------+---+----+----------+\n|  Name|    Dep|Salary|Id_Based_On_Dep| Id|rank|dense_rank|\n+------+-------+------+---------------+---+----+----------+\n| pooja|Backend| 33000|              1|  1|   1|         1|\n|   raj|Backend| 33000|              2|  2|   1|         1|\n| kumar|  Cloud| 32000|              1|  3|   3|         2|\n|  bala|  Cloud| 44000|              2|  4|   3|         2|\n|   jon|  Cloud| 44000|              3|  5|   3|         2|\n| vinay|     HR| 30000|              1|  6|   6|         3|\n|   sai|     HR| 30000|              2|  7|   6|         3|\n|   sam|     HR| 42000|              3|  8|   6|         3|\n|kumari|Manager|120000|              1|  9|   9|         4|\n+------+-------+------+---------------+---+----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# This are the ranking functions that are used upon the partitioned data using Window.partitionBy() function and for row\n",
    "# Number and rank function we need to additionally order by on partition data using orderBy clause !\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import *\n",
    "\n",
    "data = [('vinay','HR',30000),('kumar','Cloud',32000),('sai','HR',30000),('bala','Cloud',44000),('pooja','Backend',33000),('kumari','Manager',120000),('jon','Cloud',44000),('sam','HR',42000),('raj','Backend',33000)]\n",
    "sch = ['Name','Dep','Salary']\n",
    "df = spark.createDataFrame(data,sch)\n",
    "df.show(truncate = False)\n",
    "df.printSchema()\n",
    "# df= df.withColumn('Id', monotonically_increasing_id()+1)\n",
    "\n",
    "# Using the window() function and orderBy() to create a window and use that window with row_number() function to give a unique id ! to each row !\n",
    "window_spec = Window().orderBy(df.Dep)\n",
    "df.withColumn('Id',row_number().over(window_spec)).show()\n",
    "\n",
    "dd = Window.partitionBy(df.Dep).orderBy(df.Salary)\n",
    "df = df.withColumn('Id_Based_On_Dep',row_number().over(dd))\n",
    "df.show()\n",
    "# Now we will use rank() and dense_rank() functions\n",
    "df=df.withColumn('Id',row_number().over(Window.orderBy(F.lit('A'))))\n",
    "df  = df.withColumn('rank',rank().over(Window.orderBy('Dep')))\n",
    "df.show()\n",
    "\n",
    "df  = df.withColumn('dense_rank',dense_rank().over(Window.orderBy('Dep')))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9104e1b1-0585-4630-8574-fb7f77938333",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+-------------+----------------+--------------+------+------+\n|Name   |Department|Salary|AverageSalary|RankInDepartment|TotalEmployees|avg   |avguu |\n+-------+----------+------+-------------+----------------+--------------+------+------+\n|John   |HR        |5000  |5500.0       |3               |3             |5000.0|5000.0|\n|Charlie|HR        |5500  |5750.0       |2               |2             |5250.0|5250.0|\n|Alice  |HR        |6000  |6000.0       |1               |1             |5500.0|5500.0|\n|Bob    |IT        |7000  |7500.0       |2               |2             |5875.0|5875.0|\n|Eve    |IT        |8000  |8000.0       |1               |1             |6300.0|6300.0|\n+-------+----------+------+-------------+----------------+--------------+------+------+\n\n+-------+----------+------+-------------+----------------+--------------+------+------+------------+\n|   Name|Department|Salary|AverageSalary|RankInDepartment|TotalEmployees|   avg| avguu|Over_all_Avg|\n+-------+----------+------+-------------+----------------+--------------+------+------+------------+\n|   John|        HR|  5000|       5500.0|               3|             3|5000.0|5000.0|      6300.0|\n|Charlie|        HR|  5500|       5750.0|               2|             2|5250.0|5250.0|      6300.0|\n|  Alice|        HR|  6000|       6000.0|               1|             1|5500.0|5500.0|      6300.0|\n|    Bob|        IT|  7000|       7500.0|               2|             2|5875.0|5875.0|      6300.0|\n|    Eve|        IT|  8000|       8000.0|               1|             1|6300.0|6300.0|      6300.0|\n+-------+----------+------+-------------+----------------+--------------+------+------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"window_example\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"John\", \"HR\", 5000),\n",
    "    (\"Alice\", \"HR\", 6000),\n",
    "    (\"Bob\", \"IT\", 7000),\n",
    "    (\"Eve\", \"IT\", 8000),\n",
    "    (\"Charlie\", \"HR\", 5500),\n",
    "]\n",
    "\n",
    "# Define schema\n",
    "schema = [\"Name\", \"Department\", \"Salary\"]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Define a Window specification partitioned by 'Department' and ordered by 'Salary'\n",
    "window_spec = Window().partitionBy(\"Department\").orderBy(F.desc(\"Salary\"))\n",
    "\n",
    "# Add columns for average salary, individual salary, and rank within each department\n",
    "df_result = (\n",
    "    df.withColumn(\"AverageSalary\", F.avg(\"Salary\").over(window_spec))\n",
    "    .withColumn(\"RankInDepartment\", F.row_number().over(window_spec))\n",
    "    .withColumn(\"TotalEmployees\", F.count(\"Name\").over(window_spec))\n",
    "    .withColumn(\"avg\", F.avg(\"Salary\").over(Window.orderBy(F.asc(\"Salary\"))))\n",
    "    .withColumn(\"avguu\", F.avg(\"Salary\").over(Window.orderBy(\"Salary\")))\n",
    ")\n",
    "\n",
    "\n",
    "# Show the result\n",
    "df_result.show(truncate=False)\n",
    "overall_avg_salary = df.select(F.avg(\"Salary\")).first()[0]\n",
    "\n",
    "\n",
    "df_result.withColumn(\"Over_all_Avg\", F.lit(overall_avg_salary)).show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [
    {
     "elements": [],
     "globalVars": {},
     "guid": "",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "6399c8a1-d7c8-4be8-86e3-d45a63b7d6b7",
     "origId": 2676587401561542,
     "title": "Untitled",
     "version": "DashboardViewV1",
     "width": 1024
    }
   ],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "LEARNING_PYSPARK!",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
